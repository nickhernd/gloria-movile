{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOAD DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbJecdT555QS",
        "outputId": "18118a60-0d43-4982-ba3b-c867f35ad552"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "test_rout=[]\n",
        "test_label=[]\n",
        "fish_type=[]\n",
        "carpetas = [r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\D_labrax\\test\\C\", r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\D_labrax\\test\\S\", r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\S_Aurata\\test\\C\",r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\S_Aurata\\test\\S\"]\n",
        "for carpeta in carpetas:\n",
        "  if carpeta[-1]=='C':\n",
        "    label=[0] * len(os.listdir(carpeta))\n",
        "  elif carpeta[-1]=='S':\n",
        "    label=[1] * len(os.listdir(carpeta))\n",
        "  else:\n",
        "    label=[2] * len(os.listdir(carpeta))\n",
        "  test_label.extend(label)\n",
        "  fish_type.extend(carpeta[-15]*len(os.listdir(carpeta)))\n",
        "  for archivo in os.listdir(carpeta):\n",
        "    if os.path.isfile(os.path.join(carpeta, archivo)):\n",
        "      test_rout.append(carpeta+'/'+archivo)\n",
        "\n",
        "len(test_rout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ad5INNee04q",
        "outputId": "8de2203e-6d85-46df-a3c9-92a166fa67a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train_rout=[]\n",
        "train_label=[]\n",
        "fish_type_train=[]\n",
        "\n",
        "carpetas = [r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\D_labrax\\train\\C\", r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\D_labrax\\train\\S\", r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\S_Aurata\\train\\C\",r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\S_Aurata\\train\\S\"]\n",
        "for carpeta in carpetas:\n",
        "  if carpeta[-1]=='C':\n",
        "    label=[0] * len(os.listdir(carpeta))\n",
        "  elif carpeta[-1]=='S':\n",
        "    label=[1] * len(os.listdir(carpeta))\n",
        "  else:\n",
        "    label=[2] * len(os.listdir(carpeta))\n",
        "  train_label.extend(label)\n",
        "  fish_type_train.extend(carpeta[-16]*len(os.listdir(carpeta)))\n",
        "  for archivo in os.listdir(carpeta):\n",
        "    if os.path.isfile(os.path.join(carpeta, archivo)):\n",
        "      train_rout.append(carpeta+'/'+archivo)\n",
        "\n",
        "len(train_rout)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "dict_users_train_S = {}\n",
        "filtered_ES_rout_S = [r for r, f in zip(train_rout, fish_type_train) if f == 'S']\n",
        "for idx, file_name in enumerate(filtered_ES_rout_S):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_train_S:\n",
        "        dict_users_train_S[nombre_base] = []\n",
        "    dict_users_train_S[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_train_S)\n",
        "\n",
        "dict_users_train_D = {}\n",
        "filtered_ES_rout_D = [r for r, f in zip(train_rout, fish_type_train) if f == 'D']\n",
        "for idx, file_name in enumerate(filtered_ES_rout_D):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_train_D:\n",
        "        dict_users_train_D[nombre_base] = []\n",
        "    dict_users_train_D[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_train_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "        print(f\"Usando GPU: {physical_devices}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configurando GPU: {e}\")\n",
        "else:\n",
        "    print(\"No se detect√≥ GPU, usando CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktu25HWm-wBd",
        "outputId": "3e20a3b7-df5d-4c49-d5dd-1dd3ff810772"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "testES_rout=[]\n",
        "testES_label=[]\n",
        "fish_type_ES=[]\n",
        "\n",
        "carpetas = [r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\D_labrax\\E\",r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\S_Aurata\\E\"]\n",
        "for carpeta in carpetas:\n",
        "  if carpeta[-1]=='C':\n",
        "    label=[0] * len(os.listdir(carpeta))\n",
        "  elif carpeta[-1]=='S':\n",
        "    label=[1] * len(os.listdir(carpeta))\n",
        "  else:\n",
        "    label=[2] * len(os.listdir(carpeta))\n",
        "  testES_label.extend(label)\n",
        "  fish_type_ES.extend(carpeta[-10]*len(os.listdir(carpeta)))\n",
        "  for archivo in os.listdir(carpeta):\n",
        "    if os.path.isfile(os.path.join(carpeta, archivo)):\n",
        "      testES_rout.append(carpeta+'/'+archivo)\n",
        "\n",
        "dict_users_ES = {}\n",
        "\n",
        "for idx, file_name in enumerate(testES_rout):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_ES:\n",
        "        dict_users_ES[nombre_base] = []\n",
        "    dict_users_ES[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_ES)\n",
        "\n",
        "dict_users_ES_S = {}\n",
        "filtered_ES_rout_S = [r for r, f in zip(testES_rout, fish_type_ES) if f == 'S']\n",
        "for idx, file_name in enumerate(filtered_ES_rout_S):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_ES_S:\n",
        "        dict_users_ES_S[nombre_base] = []\n",
        "    dict_users_ES_S[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_ES_S)\n",
        "\n",
        "dict_users_ES_D = {}\n",
        "filtered_ES_rout_D = [r for r, f in zip(testES_rout, fish_type_ES) if f == 'D']\n",
        "for idx, file_name in enumerate(filtered_ES_rout_D):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_ES_D:\n",
        "        dict_users_ES_D[nombre_base] = []\n",
        "    dict_users_ES_D[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_ES_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gknw3OY6if-",
        "outputId": "32e993dd-ac9f-480f-811b-296ba26b43b8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "dict_users = {}\n",
        "\n",
        "for idx, file_name in enumerate(test_rout):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users:\n",
        "        dict_users[nombre_base] = []\n",
        "    dict_users[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "dict_users_S = {}\n",
        "filtered_test_rout_S = [r for r, f in zip(test_rout, fish_type) if f == 'S']\n",
        "for idx, file_name in enumerate(filtered_test_rout_S):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_S:\n",
        "        dict_users_S[nombre_base] = []\n",
        "    dict_users_S[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_S)\n",
        "\n",
        "dict_users_D = {}\n",
        "filtered_test_rout_D = [r for r, f in zip(test_rout, fish_type) if f == 'D']\n",
        "for idx, file_name in enumerate(filtered_test_rout_D):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_D:\n",
        "        dict_users_D[nombre_base] = []\n",
        "    dict_users_D[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLIP LUBINA (D_LABRAX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "results=[]\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "test_label_D=[]\n",
        "imagens_tensor_D = []\n",
        "for r, fish, l in zip(test_rout, fish_type, test_label):\n",
        "  if fish == \"D\":\n",
        "    image = Image.open(r)\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "    imagens_tensor_D.append(image_input)\n",
        "    test_label_D.append(l)\n",
        "\n",
        "# 84%\n",
        "'''\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side and an oval body and in which the mouth slightly tilted upwards,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, a color that transitions from a darker upper side, to a white ventral side and in which the mouth follows the same direction as the main axis of the body\", # Wild\n",
        "]\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side and an oval body and in which the mouth slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, a color that transitions from a darker upper side to a white ventral side and in which the mouth follows the same direction as the main axis of the body\", # Wild\n",
        "]\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side and an oval body and in which the mouth slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, a color that transitions from a darker upper side to a white ventral side and in which the mouth follows the same direction as the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "'''\n",
        "# 85%\n",
        "'''\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side and an oval body and in which the mouth slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, the color transitions from a darker upper side to a white ventral side and in which the mouth follows direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "'''\n",
        "\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side, an oval body and the mouth is slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, the color transitions from a darker upper side to a white ventral side and in which the mouth follows direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side, an oval body and the mouth is slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, the color transitions from a darker upper side to a white ventral side and in which the mouth follows direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "labels = [\"Farm fish\", \"Wild fish\"]\n",
        "\n",
        "\n",
        "text_inputs = torch.cat([clip.tokenize(c) for c in classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "  text_features = model.encode_text(text_inputs)\n",
        "\n",
        "batch_images = torch.cat(imagens_tensor_D, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_features_D = model.encode_image(batch_images)\n",
        "\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "for imgF in image_features_D:\n",
        "  imgF_norm = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * imgF_norm @ text_features.T).softmax(dim=-1)\n",
        "  values, indices = similarity.topk(2)\n",
        "  results.append([indices.tolist()[0],values.tolist()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred= [fila[0] for fila in results]\n",
        "print(\"\\nReporte de clasificaci√≥n:\\n\", classification_report(test_label_D, y_pred))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(test_label_D, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "users_test_label_D=[]\n",
        "users_y_pred_D=[]\n",
        "user_y_prob_D=[]\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "  users_test_label_D.append(round(sum([test_label_D[i] for i in indices])/len(indices)))\n",
        "  prob=[]\n",
        "  for z in [results[i] for i in indices]:\n",
        "    if z[0]==1:\n",
        "      prob.append(z[1])\n",
        "    else:\n",
        "      prob.append(1-z[1])\n",
        "  users_y_pred_D.append(round(sum(prob)/len(indices)))\n",
        "  user_y_prob_D.append((sum(prob)/len(indices)))\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nReporte de clasificaci√≥n:\\n\", classification_report(users_test_label_D, users_y_pred_D))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(users_test_label_D, users_y_pred_D)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLIP DORADA (S_AURATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LUBINA\n",
        "\n",
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "results=[]\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "test_label_S=[]\n",
        "imagens_tensor_S = []\n",
        "for r, fish, l in zip(test_rout, fish_type, test_label):\n",
        "  if fish == \"S\":\n",
        "    image = Image.open(r)\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "    imagens_tensor_S.append(image_input)\n",
        "    test_label_S.append(l)\n",
        "\n",
        "# 80%\n",
        "'''\n",
        "classes = [\n",
        "    \"a close-up of a dark grey fish with a curved bottom, the lateral fins are close to the body and small\",  # Captivity\n",
        "    \"a close-up of a fish with a flat bottom and in mouth follows the direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "'''\n",
        "'''\n",
        "# 84 %\n",
        "classes = [\n",
        "    \"a close-up of a dark grey fish with a curved bottom, the lateral fins are close to the body and small\",  # Captivity\n",
        "    \"a close-up of a fish with a flat bottom and in mouth follows the direction of the main axis of the body,the lateral fins separated from the body and oriented slightly backward and the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "'''\n",
        "classes = [\n",
        "    \"a close-up of a dark grey fish with an oval body, a white ventral side with high contrast with the body, the mouth is slightly tilted upwards, the lateral fins are close to the body and small\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, a white ventral side with high contrast with the body, the mouth follows the direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "classes = [\n",
        "    \"a close-up of a dark grey fish with a curved bottom, the lateral fins are close to the body and small\",  # Captivity\n",
        "    \"a close-up of a fish with a flat bottom and in mouth follows the direction of the main axis of the body, the lateral fins separated from the body and oriented slightly backward and the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "labels=[\"Farmed fish\",\"Wild fish\"]\n",
        "\n",
        "text_inputs = torch.cat([clip.tokenize(c) for c in classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "  text_features = model.encode_text(text_inputs)\n",
        "\n",
        "batch_images = torch.cat(imagens_tensor_S, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_features_S = model.encode_image(batch_images)\n",
        "\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "for imgF in image_features_S:\n",
        "  imgF_norm = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * imgF_norm @ text_features.T).softmax(dim=-1)\n",
        "  values, indices = similarity.topk(2)\n",
        "  results.append([indices.tolist()[0],values.tolist()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred= [fila[0] for fila in results]\n",
        "print(\"\\nReporte de clasificaci√≥n:\\n\", classification_report(test_label_S, y_pred))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(test_label_S, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "users_test_label_S=[]\n",
        "users_y_pred_S=[]\n",
        "user_y_prob_S=[]\n",
        "\n",
        "for _, indices in dict_users_S.items():\n",
        "  users_test_label_S.append(round(sum([test_label_S[i] for i in indices])/len(indices)))\n",
        "  prob=[]\n",
        "  for z in [results[i] for i in indices]:\n",
        "    if z[0]==1:\n",
        "      prob.append(z[1])\n",
        "    else:\n",
        "      prob.append(1-z[1])\n",
        "  users_y_pred_S.append(round(sum(prob)/len(indices)))\n",
        "  user_y_prob_S.append((sum(prob)/len(indices)))\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nReporte de clasificaci√≥n:\\n\", classification_report(users_test_label_S, users_y_pred_S))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(users_test_label_S, users_y_pred_S)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFS1aw-LPK3_"
      },
      "source": [
        "## Para cada especie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di6nE62MPPB9",
        "outputId": "b9e7e930-2107-4015-d227-3255b98c4c7e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "labels=[\"Fish in captivity\",\"Wild fish\"]\n",
        "\n",
        "device = \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "imagens_tensor_SC = []\n",
        "imagens_tensor_DC = []\n",
        "\n",
        "train_rout_C = [rout for rout, l in zip(train_rout, train_label) if l == 0]\n",
        "fish_type_train_C = [fish for fish, l in zip(fish_type_train, train_label) if l == 0]\n",
        "\n",
        "for r, fish_t in zip(train_rout_C, fish_type_train_C):\n",
        "  image = Image.open(r)\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  if fish_t == 'S':\n",
        "    imagens_tensor_SC.append(image_input)\n",
        "  elif fish_t == 'D':\n",
        "    imagens_tensor_DC.append(image_input)\n",
        "\n",
        "batch_images_SC = torch.cat(imagens_tensor_SC, dim=0)\n",
        "batch_images_DC = torch.cat(imagens_tensor_DC, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_features_SC = model.encode_image(batch_images_SC)\n",
        "  image_features_DC = model.encode_image(batch_images_DC)\n",
        "\n",
        "imagens_tensor_SS = []\n",
        "imagens_tensor_DS = []\n",
        "\n",
        "train_rout_S = [rout for rout, l in zip(train_rout, train_label) if l == 1]\n",
        "fish_type_train_S = [fish for fish, l in zip(fish_type_train, train_label) if l == 1]\n",
        "\n",
        "for r, fish_t in zip(train_rout_S, fish_type_train_S):\n",
        "  image = Image.open(r)\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  if fish_t == 'S':\n",
        "    imagens_tensor_SS.append(image_input)\n",
        "  elif fish_t == 'D':\n",
        "    imagens_tensor_DS.append(image_input)\n",
        "\n",
        "batch_images_SS = torch.cat(imagens_tensor_SS, dim=0)\n",
        "batch_images_DS = torch.cat(imagens_tensor_DS, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_features_SS = model.encode_image(batch_images_SS)\n",
        "  image_features_DS = model.encode_image(batch_images_DS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imagens_tensor_ES=[]\n",
        "imagens_tensor_ED=[]\n",
        "\n",
        "device = \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "for fish_t, r in zip(fish_type_ES,testES_rout):\n",
        "  image = Image.open(r)\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  if fish_t == 'S':\n",
        "    imagens_tensor_ES.append(image_input)\n",
        "  elif fish_t == 'D':\n",
        "    imagens_tensor_ED.append(image_input)\n",
        "batch_images_ED = torch.cat(imagens_tensor_ED, dim=0)\n",
        "batch_images_ES = torch.cat(imagens_tensor_ES, dim=0)\n",
        "with torch.no_grad():\n",
        "  image_features_ES = model.encode_image(batch_images_ES)\n",
        "  image_features_ED = model.encode_image(batch_images_ED)\n",
        "\n",
        "text_labels_S = [\n",
        "    \"a close-up of a dark grey fish with a curved bottom, the lateral fins are close to the body and small\",  # Captivity\n",
        "    \"a close-up of a fish with a flat bottom and in mouth follows the direction of the main axis of the body, the lateral fins separated from the body and oriented slightly backward and the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_S]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_S = model.encode_text(text_inputs)\n",
        "\n",
        "text_features_S_norm = text_features_S / text_features_S.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_labels_D = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side, an oval body and the mouth is slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, the color transitions from a darker upper side to a white ventral side and in which the mouth follows direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_D]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_D = model.encode_text(text_inputs)\n",
        "\n",
        "text_features_D_norm = text_features_D / text_features_D.norm(dim=-1, keepdim=True)\n",
        "\n",
        "all_features_S=[]\n",
        "for imgF in image_features_ES:\n",
        "    imgF2 = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "    similarity = 100.0 * imgF2 @ text_features_S_norm.T  \n",
        "    values, indices = similarity.topk(2)\n",
        "    all_features_S.append([imgF,text_features_S[indices[0]]])\n",
        "\n",
        "all_features_D=[]\n",
        "for imgF in image_features_ED:\n",
        "    imgF2 = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "    similarity = 100.0 * imgF2 @ text_features_D_norm.T  \n",
        "    values, indices = similarity.topk(2)\n",
        "    all_features_D.append([imgF,text_features_D[indices[0]]])\n",
        "\n",
        "XEscap_S = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in all_features_S]\n",
        "XEscap_D = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in all_features_D]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zChSStNITLHS"
      },
      "outputs": [],
      "source": [
        "comb_image_features_SC = torch.mean(image_features_SC, dim=0)\n",
        "comb_image_features_DC = torch.mean(image_features_DC, dim=0)\n",
        "comb_image_features_SS = torch.mean(image_features_SS, dim=0)\n",
        "comb_image_features_DS = torch.mean(image_features_DS, dim=0)\n",
        "\n",
        "comb_image_features_SC /= comb_image_features_SC.norm(dim=-1, keepdim=True)\n",
        "comb_image_features_DC /= comb_image_features_DC.norm(dim=-1, keepdim=True)\n",
        "comb_image_features_SS /= comb_image_features_SS.norm(dim=-1, keepdim=True)\n",
        "comb_image_features_DS /= comb_image_features_DS.norm(dim=-1, keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwOzjsoCTprA",
        "outputId": "32df9068-29fc-437a-f91a-dfcbdd816096"
      },
      "outputs": [],
      "source": [
        "alpha=0.5\n",
        "\n",
        "results_S=[]\n",
        "results_D=[]\n",
        "\n",
        "test_imagens_tensor_S = []\n",
        "test_imagens_tensor_D = []\n",
        "test_label_S=[]\n",
        "test_label_D=[]\n",
        "\n",
        "text_labels_S = [\n",
        "    \"A close-up of a fish where the lateral line is barely visible and fragmented\",\n",
        "    \"A close-up of a fish with a slightly curved mouth\",\n",
        "    \"A close-up of a fish with a light underside sharply contrasting its darker grey body.\",\n",
        "    \"A close-up of a fish characterized by a consistently structured and evident lateral line\",\n",
        "    \"A close-up of a fish with a mouth maintaining a level alignment\",\n",
        "    \"A close-up of a fish with a bright grey body transitioning to white underneath.\",\n",
        "]\n",
        "\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_S]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_S = model.encode_text(text_inputs)\n",
        "\n",
        "text_features_S_norm = text_features_S / text_features_S.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_labels_D = [\n",
        "    # Cultivada\n",
        "    \"A close-up of a fish with an even transition from snout to lips\",\n",
        "    \"A close-up of a fish where the lateral line is barely visible and fragmented\",\n",
        "    \"A close-up of a fish with a dorsal fin that is worn out.\",\n",
        "\n",
        "    # Salvaje\n",
        "    \"A close-up of a fish with a pronounced drop from forehead to lips\",\n",
        "    \"A close-up of a fish where the lateral line appears continuous and well-marked\",\n",
        "    \"A close-up of a fish with fins that are not close to the body\",\n",
        "]\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_D]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_D = model.encode_text(text_inputs)\n",
        "\n",
        "text_features_D_norm = text_features_D / text_features_D.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "for r, fish_t, lab_t in zip(test_rout, fish_type, test_label):\n",
        "  image = Image.open(r)\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  if fish_t == 'S':\n",
        "    test_imagens_tensor_S.append(image_input)\n",
        "    test_label_S.append(lab_t)\n",
        "  elif fish_t == 'D':\n",
        "    test_imagens_tensor_D.append(image_input)\n",
        "    test_label_D.append(lab_t)\n",
        "\n",
        "test_batch_images_S = torch.cat(test_imagens_tensor_S, dim=0)\n",
        "test_batch_images_D = torch.cat(test_imagens_tensor_D, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  test_image_features_S = model.encode_image(test_batch_images_S)\n",
        "  test_image_features_D = model.encode_image(test_batch_images_D)\n",
        "\n",
        "for imgF in test_image_features_S:\n",
        "    imgF2 = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Similaridad con im√°genes y texto\n",
        "    similarity_C_image = 100.0 * imgF2 @ comb_image_features_SC.T\n",
        "    similarity_S_image = 100.0 * imgF2 @ comb_image_features_SS.T\n",
        "\n",
        "    similarity_C_text = (100.0 * imgF2 @ text_features_S_norm[:3].T).mean(dim=-1)\n",
        "    similarity_S_text = (100.0 * imgF2 @ text_features_S_norm[3:].T).mean(dim=-1)  \n",
        "\n",
        "    similarity_C_combined = alpha * similarity_C_image + (1 - alpha) * similarity_C_text\n",
        "    similarity_S_combined = alpha * similarity_S_image + (1 - alpha) * similarity_S_text\n",
        "\n",
        "    # Combinar las similitudes (im√°genes y texto)\n",
        "    similarity_combined = torch.cat(\n",
        "        (\n",
        "            similarity_C_combined.unsqueeze(0),\n",
        "            similarity_S_combined.unsqueeze(0)\n",
        "        ), \n",
        "        dim=-1\n",
        "    )\n",
        "\n",
        "    # Aplicar softmax para obtener probabilidades\n",
        "    similarity = similarity_combined.softmax(dim=-1)\n",
        "    values, indices = similarity.topk(2)\n",
        "    results_S.append([indices.tolist()[0], values.tolist()[0]])\n",
        "\n",
        "for imgF in test_image_features_D:\n",
        "    imgF2 = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Similaridad con im√°genes y texto\n",
        "    similarity_C_image = 100.0 * imgF2 @ comb_image_features_DC.T\n",
        "    similarity_S_image = 100.0 * imgF2 @ comb_image_features_DS.T\n",
        "\n",
        "    similarity_C_text = (100.0 * imgF2 @ text_features_D_norm[:3].T).mean(dim=-1)\n",
        "    similarity_S_text = (100.0 * imgF2 @ text_features_D_norm[3:].T).mean(dim=-1)  \n",
        "\n",
        "    similarity_C_combined = alpha * similarity_C_image + (1 - alpha) * similarity_C_text\n",
        "    similarity_S_combined = alpha * similarity_S_image + (1 - alpha) * similarity_S_text\n",
        "\n",
        "    # Combinar las similitudes (im√°genes y texto)\n",
        "    similarity_combined = torch.cat(\n",
        "        (\n",
        "            similarity_C_combined.unsqueeze(0),\n",
        "            similarity_S_combined.unsqueeze(0)\n",
        "        ), \n",
        "        dim=-1\n",
        "    )\n",
        "\n",
        "\n",
        "    # Aplicar softmax para obtener probabilidades\n",
        "    similarity = similarity_combined.softmax(dim=-1)\n",
        "    values, indices = similarity.topk(2)\n",
        "    results_D.append([indices.tolist()[0], values.tolist()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXtNkbpPXWUM",
        "outputId": "a8149e0f-dcf6-4b9f-e40a-6261dc99cdda"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred_S= [fila[0] for fila in results_S]\n",
        "test_label_S = [int(label) for label in test_label_S]\n",
        "assert len(y_pred_S) == len(test_label_S)\n",
        "print(\"\\nReporte de clasificaci√≥n en S_Aurata:\\n\", classification_report(test_label_S, y_pred_S))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCXPyS1bXtzf",
        "outputId": "9003d341-2943-43cf-95fc-d07dc431f5ec"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred_D= [fila[0] for fila in results_D]\n",
        "print(\"\\nReporte de clasificaci√≥n en D_labrax:\\n\", classification_report(test_label_D, y_pred_D))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALJeTwqKXis1"
      },
      "outputs": [],
      "source": [
        "users_test_label_S=[]\n",
        "users_y_pred_S=[]\n",
        "user_y_prob_S=[]\n",
        "\n",
        "for _, indices in dict_users_S.items():\n",
        "  users_test_label_S.append(sum([test_label_S[i] for i in indices])/len(indices))\n",
        "\n",
        "  prob=[]\n",
        "  for z in [results_S[i] for i in indices]:\n",
        "    if z[0]==1:\n",
        "      prob.append(z[1])\n",
        "    else:\n",
        "      prob.append(1-z[1])\n",
        "  users_y_pred_S.append(round(sum(prob)/len(indices)))\n",
        "  user_y_prob_S.append((sum(prob)/len(indices)))\n",
        "\n",
        "users_test_label_D=[]\n",
        "users_y_pred_D=[]\n",
        "user_y_prob_D=[]\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "  users_test_label_D.append(sum([test_label_D[i] for i in indices])/len(indices))\n",
        "\n",
        "  prob=[]\n",
        "  for z in [results_D[i] for i in indices]:\n",
        "    if z[0]==1:\n",
        "      prob.append(z[1])\n",
        "    else:\n",
        "      prob.append(1-z[1])\n",
        "  users_y_pred_D.append(round(sum(prob)/len(indices)))\n",
        "  user_y_prob_D.append((sum(prob)/len(indices)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK9aC8dsd3j4",
        "outputId": "3f70da93-2cf7-475b-a5fe-5d9a1b40532c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nReporte de clasificaci√≥n:\\n\", classification_report(users_test_label_S, users_y_pred_S))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoQQS3KPd4D7",
        "outputId": "c1524a2e-74f8-471c-d686-d282ad5d46d3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nReporte de clasificaci√≥n:\\n\", classification_report(users_test_label_D, users_y_pred_D))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "qXjO3AahdxFi",
        "outputId": "5d1f1388-3ac1-459c-b810-c49396def59a"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(users_test_label_S, users_y_pred_S)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix S_aurata')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "eGGYiB2reMcE",
        "outputId": "7cdd9816-9bce-4b0b-f5f8-35110354542a"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(users_test_label_D, users_y_pred_D)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix D_labrax')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T-SNE AURATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "5egfwoLyesM_",
        "outputId": "ccb5cc29-8ad5-4afd-8be7-bd03c2a3243e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "embeddings_S = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "text_features_S_extended = np.vstack((text_features_S[0].unsqueeze(0).numpy(),\n",
        "                                      text_features_S[1].unsqueeze(0).numpy()))\n",
        "embeddings_S_Text = np.concatenate((embeddings_S, text_features_S_extended), axis=0)\n",
        "embeddings_S_test = test_image_features_S\n",
        "\n",
        "labels_S = np.concatenate((np.zeros(len(image_features_SS)),  # Aurata S (im√°genes)\n",
        "                         np.ones(len(image_features_SC)),   # Aurata C (im√°genes)\n",
        "                         [2, 3]))  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "labels_test = test_label_S\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "embeddings_S_tsne = tsne.fit_transform(embeddings_S_Text)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Clase 0: Aurata S\n",
        "plt.scatter(embeddings_S_tsne[(labels_S == 0), 0], embeddings_S_tsne[(labels_S == 0), 1],\n",
        "            c='blue', marker='o', label='Aurata S')\n",
        "\n",
        "# Clase 1: Aurata C\n",
        "plt.scatter(embeddings_S_tsne[(labels_S == 1), 0], embeddings_S_tsne[(labels_S == 1), 1],\n",
        "            c='green', marker='o', label='Aurata C')\n",
        "\n",
        "# Clase 0: Aurata S\n",
        "plt.scatter(embeddings_S_tsne[(labels_S == 2), 0], embeddings_S_tsne[(labels_S == 2), 1],\n",
        "            c='red', marker='s', label='EmbTexto S')\n",
        "\n",
        "# Clase 1: Aurata C\n",
        "plt.scatter(embeddings_S_tsne[(labels_S == 3), 0], embeddings_S_tsne[(labels_S == 3), 1],\n",
        "            c='red', marker='s', label='EmbTexto C')\n",
        "\n",
        "plt.title(\"t-SNE de los embeddings en 2D\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import MDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "embeddings_S = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "text_features_S_extended = np.vstack((text_features_S[0].unsqueeze(0).numpy(),\n",
        "                                      text_features_S[1].unsqueeze(0).numpy()))\n",
        "embeddings_S_Text = np.concatenate((embeddings_S, text_features_S_extended), axis=0)\n",
        "\n",
        "labels_S = np.concatenate((\n",
        "    np.zeros(len(image_features_SS)),  # Clase 0: Aurata S\n",
        "    np.ones(len(image_features_SC)),   # Clase 1: Aurata C\n",
        "    [2, 3]  # Clase 2: Embedding Texto S, Clase 3: Embedding Texto C\n",
        "))\n",
        "\n",
        "# Aplicar MDS\n",
        "mds = MDS(n_components=2, random_state=42)\n",
        "embeddings_S_mds = mds.fit_transform(embeddings_S_Text)\n",
        "\n",
        "# Graficar MDS\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Clase 0: Aurata S\n",
        "plt.scatter(embeddings_S_mds[(labels_S == 0), 0], embeddings_S_mds[(labels_S == 0), 1],\n",
        "            c='blue', marker='o', label='Aurata S')\n",
        "\n",
        "# Clase 1: Aurata C\n",
        "plt.scatter(embeddings_S_mds[(labels_S == 1), 0], embeddings_S_mds[(labels_S == 1), 1],\n",
        "            c='green', marker='o', label='Aurata C')\n",
        "\n",
        "# Clase 2: Embedding Texto S\n",
        "plt.scatter(embeddings_S_mds[(labels_S == 2), 0], embeddings_S_mds[(labels_S == 2), 1],\n",
        "            c='red', marker='s', label='EmbTexto S')\n",
        "\n",
        "# Clase 3: Embedding Texto C\n",
        "plt.scatter(embeddings_S_mds[(labels_S == 3), 0], embeddings_S_mds[(labels_S == 3), 1],\n",
        "            c='orange', marker='s', label='EmbTexto C')\n",
        "\n",
        "plt.title(\"MDS de los embeddings en 2D\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "#text_features_D /= np.linalg.norm(text_features_D, axis=1, keepdims=True)\n",
        "\n",
        "embeddings_D = np.concatenate((image_features_DS, image_features_DC), axis=0)\n",
        "text_features_D_extended = np.vstack((text_features_D[0].unsqueeze(0).numpy(),\n",
        "                                      text_features_D[1].unsqueeze(0).numpy()))\n",
        "embeddings_D_Text = np.concatenate((embeddings_D, text_features_D_extended), axis=0)\n",
        "embeddings_D_test = test_image_features_D\n",
        "\n",
        "labels_D = np.concatenate((np.zeros(len(image_features_DS)),  # Aurata S (im√°genes)\n",
        "                         np.ones(len(image_features_DC)),   # Aurata C (im√°genes)\n",
        "                         [2, 3]))  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "labels_test = test_label_D\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "embeddings_D_tsne = tsne.fit_transform(embeddings_D_Text)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Clase 0: Aurata S\n",
        "plt.scatter(embeddings_D_tsne[(labels_D == 0), 0], embeddings_D_tsne[(labels_D == 0), 1],\n",
        "            c='blue', marker='o', label='Dorada S')\n",
        "\n",
        "# Clase 1: Aurata C\n",
        "plt.scatter(embeddings_D_tsne[(labels_D == 1), 0], embeddings_D_tsne[(labels_D == 1), 1],\n",
        "            c='green', marker='o', label='Doarada C')\n",
        "\n",
        "# Clase 0: Aurata S\n",
        "plt.scatter(embeddings_D_tsne[(labels_D == 2), 0], embeddings_D_tsne[(labels_D == 2), 1],\n",
        "            c='red', marker='s', label='EmbTexto S')\n",
        "\n",
        "# Clase 1: Aurata C\n",
        "plt.scatter(embeddings_D_tsne[(labels_D == 3), 0], embeddings_D_tsne[(labels_D == 3), 1],\n",
        "            c='red', marker='s', label='EmbTexto C')\n",
        "\n",
        "plt.title(\"t-SNE de los embeddings en 2D\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_labels_S_ref = [\n",
        "    \"a close-up of a cultivated fish\",  # Captivity\n",
        "    \"a close-up of a wild fish\", # Wild\n",
        "]\n",
        "text_labels_D_ref = [\n",
        "    \"a close-up of a cultivated fish\",  # Captivity\n",
        "    \"a close-up of a wild fish\", # Wild\n",
        "]\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_S_ref]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_S_ref = model.encode_text(text_inputs)\n",
        "\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_D_ref]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_D_ref = model.encode_text(text_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedings_DText=[]\n",
        "embeddings_DText_test=[]\n",
        "\n",
        "embeddings_D_tensor = torch.tensor(embeddings_D)\n",
        "embeddings_D_test_tensor = torch.tensor(embeddings_D_test)\n",
        "\n",
        "text_norm_D = text_features_D_norm\n",
        "\n",
        "for emb in embeddings_D_tensor:\n",
        "  emb_norm = emb / emb.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * emb_norm @ text_norm_D.T).softmax(dim=-1)\n",
        "  similarity_class_0 = similarity[:3].mean()  # Average similarity for class 0 (Cultivada)\n",
        "  similarity_class_1 = similarity[3:].mean()  # Average similarity for class 1 (Salvaje)\n",
        "  class_similarities = torch.tensor([similarity_class_0, similarity_class_1])\n",
        "  hola, indices = class_similarities.topk(1)\n",
        "  most_similar_text_embedding = text_features_D_ref[indices[0]]\n",
        "  embedings_DText.append((emb, most_similar_text_embedding))\n",
        "\n",
        "for emb in embeddings_D_test:\n",
        "  emb_norm = emb / emb.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * emb_norm @ text_norm_D.T).softmax(dim=-1)\n",
        "  similarity_class_0 = similarity[:3].mean()  # Average similarity for class 0 (Cultivada)\n",
        "  similarity_class_1 = similarity[3:].mean()  # Average similarity for class 1 (Salvaje)\n",
        "  class_similarities = torch.tensor([similarity_class_0, similarity_class_1])\n",
        "  _, indices = class_similarities.topk(1)\n",
        "  most_similar_text_embedding = text_features_D_ref[indices[0]]\n",
        "  embeddings_DText_test.append((emb, most_similar_text_embedding))\n",
        "\n",
        "embedings_SText = []\n",
        "embeddings_SText_test = []\n",
        "\n",
        "# Assuming `embeddings_S` and `embeddings_S_test` are defined tensors\n",
        "embeddings_S_tensor = torch.tensor(embeddings_S)\n",
        "embeddings_S_test_tensor = torch.tensor(embeddings_S_test)\n",
        "\n",
        "# Normalized text features\n",
        "text_norm_S = text_features_S_norm\n",
        "\n",
        "# Processing embeddings for training\n",
        "for emb in embeddings_S_tensor:\n",
        "    emb_norm = emb / emb.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * emb_norm @ text_norm_S.T).softmax(dim=-1)\n",
        "    similarity_class_0 = similarity[:3].mean()  # Average similarity for class 0 (Cultivada)\n",
        "    similarity_class_1 = similarity[3:].mean()  # Average similarity for class 1 (Salvaje)\n",
        "    class_similarities = torch.tensor([similarity_class_0, similarity_class_1])\n",
        "    _, indices = class_similarities.topk(1)\n",
        "    most_similar_text_embedding = text_features_S_ref[indices[0]]\n",
        "    embedings_SText.append((emb, most_similar_text_embedding))\n",
        "\n",
        "# Processing embeddings for testing\n",
        "for emb in embeddings_S_test_tensor:\n",
        "    emb_norm = emb / emb.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * emb_norm @ text_norm_S.T).softmax(dim=-1)\n",
        "    similarity_class_0 = similarity[:3].mean()  # Average similarity for class 0 (Cultivada)\n",
        "    similarity_class_1 = similarity[3:].mean()  # Average similarity for class 1 (Salvaje)\n",
        "    class_similarities = torch.tensor([similarity_class_0, similarity_class_1])\n",
        "    _, indices = class_similarities.topk(1)\n",
        "    most_similar_text_embedding = text_features_S_ref[indices[0]]\n",
        "    embeddings_SText_test.append((emb, most_similar_text_embedding))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# S_Aurata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "XTRAIN=embedings_SText\n",
        "XTEST=embeddings_SText_test\n",
        "labels_S = np.concatenate((np.zeros(len(image_features_SS)),  # Aurata S (im√°genes)\n",
        "                         np.ones(len(image_features_SC))), axis=0)  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "yTRAIN=labels_S\n",
        "yTEST=test_label_S\n",
        "\n",
        "XTRAIN = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in embedings_SText]\n",
        "XTEST = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in embeddings_SText_test]\n",
        "\n",
        "print(len(XTRAIN))\n",
        "\n",
        "yTRAIN = np.array(yTRAIN)\n",
        "yTEST = np.array(yTEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "X_test = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_neighbors': [5, 7, 10, 15, 20],  # N√∫mero de vecinos\n",
        "    'weights': ['uniform', 'distance'],  # Peso de los vecinos\n",
        "    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']  # M√©tricas de distancia\n",
        "}\n",
        "\n",
        "grid_search_knn = GridSearchCV(\n",
        "    estimator=KNeighborsClassifier(),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # Validaci√≥n cruzada\n",
        "    scoring='accuracy',  # M√©trica para evaluar\n",
        "    verbose=1,\n",
        "    n_jobs=-1  # Paralelizar la b√∫squeda\n",
        ")\n",
        "\n",
        "# Ajustar el modelo a los datos de entrenamiento\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Mejores par√°metros para KNN:\", grid_search_knn.best_params_)\n",
        "\n",
        "best_knn = grid_search_knn.best_estimator_\n",
        "y_pred_knn = best_knn.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy con el mejor KNN: {accuracy_score(y_test, y_pred_knn):.2f}\")\n",
        "print(\"Reporte de clasificaci√≥n para KNN:\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "# Aplicar t-SNE solo a los datos de entrenamiento\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "indices = np.arange(len(y_test))\n",
        "correct = (y_pred_knn == y_test)\n",
        "\n",
        "all_embeddings = np.vstack((X_train, X_test))\n",
        "all_labels = np.concatenate((y_train, y_test))\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "all_tsne = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "train_tsne = all_tsne[:len(X_train)]\n",
        "test_tsne = all_tsne[len(X_train):]\n",
        "\n",
        "# Graficar en el mismo espacio\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 0), 0], train_tsne[np.where(y_train == 0), 1],\n",
        "    c='blue', marker='o', label='Labrax C (Train)', alpha=0.5\n",
        ")\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 1), 0], train_tsne[np.where(y_train == 1), 1],\n",
        "    c='green', marker='o', label='Labrax S (Train)', alpha=0.5\n",
        ")\n",
        "\n",
        "plt.scatter(\n",
        "    test_tsne[correct, 0], test_tsne[correct, 1],\n",
        "    c='black', marker='x', label='Predicci√≥n correcta (Test)', alpha=0.8\n",
        ")\n",
        "plt.scatter(\n",
        "    test_tsne[~correct, 0], test_tsne[~correct, 1],\n",
        "    c='red', marker='x', label='Predicci√≥n incorrecta (Test)', alpha=0.8\n",
        ")\n",
        "plt.title(\"t-SNE combinado: Entrenamiento y prueba\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_labels_Sclusters = text_labels_S\n",
        "text_labels_Dclusters = text_labels_D\n",
        "\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_Sclusters]).to(device)\n",
        "text_inputs_2 = torch.cat([clip.tokenize(label) for label in text_labels_Dclusters]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_Sclusters = model.encode_text(text_inputs)\n",
        "    text_features_Dclusters = model.encode_text(text_inputs_2)\n",
        "\n",
        "text_features_Sclusters_norm = text_features_Sclusters / text_features_Sclusters.norm(dim=-1, keepdim=True)\n",
        "text_features_Dclusters_norm = text_features_Dclusters / text_features_Dclusters.norm(dim=-1, keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "color_for_user = ['yellow'] * len(XTRAIN)\n",
        "\n",
        "claves = list(dict_users_train_S.keys())\n",
        "sorted_keys = sorted(claves, key=lambda key: len(dict_users_train_S[key]), reverse=True)\n",
        "\n",
        "# Seleccionar las 5 claves m√°s grandes\n",
        "top_5_keys = sorted_keys[:5]\n",
        "print(top_5_keys)\n",
        "\n",
        "# Obtener las posiciones de esas claves en la lista original\n",
        "top_5_positions = [claves.index(key) for key in top_5_keys]\n",
        "print(claves.index('SA193'))\n",
        "\n",
        "print(top_5_positions)\n",
        "\n",
        "top_5_lengths = [len(dict_users_train_S[claves[pos]]) for pos in top_5_positions]\n",
        "print(top_5_lengths)\n",
        "\n",
        "if claves[93] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[93]]:\n",
        "        color_for_user[indice] = 'red'\n",
        "if claves[110] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[110]]:\n",
        "        color_for_user[indice] = 'blue'\n",
        "if claves[45] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[45]]:\n",
        "        color_for_user[indice] = 'green'\n",
        "if claves[16] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[16]]:\n",
        "        color_for_user[indice] = 'cyan'\n",
        "if claves[105] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[105]]:\n",
        "        color_for_user[indice] = 'purple'\n",
        "\n",
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(train_tsne)):\n",
        "    plt.scatter(train_tsne[i, 0], train_tsne[i, 1], color=color_for_user[i] ,alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import MDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Aplicar MDS a los datos de entrenamiento\n",
        "mds = MDS(n_components=2, random_state=42)\n",
        "train_mds = mds.fit_transform(X_train)\n",
        "test_mds = mds.fit_transform(X_test)\n",
        "\n",
        "# Graficar MDS\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(train_mds[y_train == 0, 0], train_mds[y_train == 0, 1], c='blue', label='Aurata C (Train)', alpha=0.5)\n",
        "plt.scatter(train_mds[y_train == 1, 0], train_mds[y_train == 1, 1], c='green', label='Aurata S (Train)', alpha=0.5)\n",
        "plt.scatter(test_mds[correct, 0], test_mds[correct, 1], c='black', marker='x', label='Predicci√≥n correcta (Test)', alpha=0.8)\n",
        "plt.scatter(test_mds[~correct, 0], test_mds[~correct, 1], c='red', marker='x', label='Predicci√≥n incorrecta (Test)', alpha=0.8)\n",
        "plt.title(\"MDS combinado: Entrenamiento y prueba\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "n_clusters = 2\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(embeddings_S)\n",
        "\n",
        "# Graficar MDS para los clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(n_clusters):\n",
        "    plt.scatter(train_mds[(clusters == i), 0], train_mds[(clusters == i), 1],\n",
        "                label=f'Cluster {i + 1}', alpha=0.5)\n",
        "\n",
        "# Calcular los centroides en el espacio MDS\n",
        "centroids_mds = np.array([\n",
        "    train_mds[(clusters == i)].mean(axis=0) for i in range(n_clusters)\n",
        "])\n",
        "\n",
        "# A√±adir los centroides\n",
        "plt.scatter(centroids_mds[:, 0], centroids_mds[:, 1], c='black', marker='x', s=100, label='Centroides')\n",
        "\n",
        "plt.title(\"Clusters k-means en el espacio MDS\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# `embedings_SText` contiene pares (embedding, text_embedding)\n",
        "# `text_labels_S` contiene los textos originales asociados a los embeddings en `text_features_S`\n",
        "\n",
        "# Convertir los datos en arrays numpy\n",
        "# Concatenar embeddings de imagen y texto antes de entrenar kmeans\n",
        "XTRAIN_array = np.array([np.concatenate((emb.numpy(), text.numpy())) for emb, text in embedings_SText])\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(XTRAIN_array)\n",
        "\n",
        "# Lista para almacenar los textos representativos\n",
        "textos_representativos = []\n",
        "\n",
        "for i in range(4):\n",
        "    # Calculamos las distancias al centroide del cl√∫ster\n",
        "    distances = pairwise_distances([kmeans.cluster_centers_[i]], XTRAIN_array)\n",
        "    closest_point_idx = np.argmin(distances)  # √çndice del punto m√°s cercano\n",
        "\n",
        "    # Recuperar el embedding de texto m√°s cercano\n",
        "    _, closest_text_embedding = embedings_SText[closest_point_idx]\n",
        "\n",
        "    # Convertir embedding a texto original buscando el √≠ndice correspondiente en `text_features_S`\n",
        "    similarity = (closest_text_embedding @ text_norm_S.T).softmax(dim=-1)\n",
        "    text_idx = similarity.argmax().item()  # √çndice del texto m√°s similar\n",
        "    texto_representativo = text_labels_S[text_idx]  # Mapea al texto original\n",
        "\n",
        "    textos_representativos.append(texto_representativo)\n",
        "\n",
        "# Mostrar los textos representativos\n",
        "print(\"Textos representativos por cl√∫ster:\")\n",
        "for i, texto in enumerate(textos_representativos, 1):\n",
        "    print(f\"Cl√∫ster {i}: {texto}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "X_test = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "# Definir grid de hiperpar√°metros para Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # Validaci√≥n cruzada\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1  # Paralelizaci√≥n\n",
        ")\n",
        "\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Mejores par√°metros para Random Forest:\", grid_search_rf.best_params_)\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy con Random Forest: {accuracy_score(y_test, y_pred_rf):.2f}\")\n",
        "print(\"Reporte de clasificaci√≥n para Random Forest:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "all_embeddings = np.vstack((X_train, X_test))\n",
        "all_labels = np.concatenate((y_train, y_test))\n",
        "all_tsne = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "train_tsne = all_tsne[:len(X_train)]\n",
        "test_tsne = all_tsne[len(X_train):]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 0), 0], train_tsne[np.where(y_train == 0), 1],\n",
        "    c='blue', marker='o', label='Aurata S (Train)', alpha=0.5\n",
        ")\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 1), 0], train_tsne[np.where(y_train == 1), 1],\n",
        "    c='green', marker='o', label='Aurata C (Train)', alpha=0.5\n",
        ")\n",
        "\n",
        "correct = (y_pred_rf == y_test)\n",
        "plt.scatter(\n",
        "    test_tsne[correct, 0], test_tsne[correct, 1],\n",
        "    c='black', marker='x', label='Predicci√≥n correcta (Test)', alpha=0.8\n",
        ")\n",
        "plt.scatter(\n",
        "    test_tsne[~correct, 0], test_tsne[~correct, 1],\n",
        "    c='red', marker='x', label='Predicci√≥n incorrecta (Test)', alpha=0.8\n",
        ")\n",
        "plt.title(\"t-SNE combinado: Entrenamiento y prueba\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "users_test_label_S=[]\n",
        "users_y_pred_S=[]\n",
        "\n",
        "for _, indices in dict_users_S.items():\n",
        "  users_test_label_S.append(sum([y_test[i] for i in indices])/len(indices))\n",
        "  users_y_pred_S.append(sum([y_pred_knn[i] for i in indices])/len(indices))\n",
        "\n",
        "# Discretizar los valores continuos\n",
        "users_test_label_S_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_S]\n",
        "users_y_pred_S_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_S]\n",
        "\n",
        "print(len(users_test_label_S_discrete))\n",
        "print(len(users_y_pred_S_discrete))\n",
        "\n",
        "# Calcular la matriz de confusi√≥n\n",
        "cm = confusion_matrix(users_test_label_S_discrete, users_y_pred_S_discrete)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix S_aurata')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Escalado de los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "# Configuraci√≥n del GridSearchCV para SVC\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],  # Valores del par√°metro de penalizaci√≥n\n",
        "    'gamma': [0.01, 0.1, 1, 'scale', 'auto'],  # Valores del coeficiente del kernel\n",
        "    'kernel': ['linear', 'rbf', 'poly']  # Tipos de kernel\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=SVC(),  # Modelo SVC\n",
        "    param_grid=param_grid, \n",
        "    cv=5,  # Validaci√≥n cruzada con 5 particiones\n",
        "    verbose=1, \n",
        "    n_jobs=-1  # Paralelizaci√≥n\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "print(\"\\nMejores par√°metros encontrados:\")\n",
        "print(best_params)\n",
        "\n",
        "model_svc = grid_search.best_estimator_\n",
        "y_pred = model_svc.predict(X_test_scaled)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy del mejor modelo: {accuracy:.2f}\")\n",
        "print(\"\\nReporte de clasificaci√≥n:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "users_test_label_S=[]\n",
        "users_y_pred_S=[]\n",
        "\n",
        "for _, indices in dict_users_S.items():\n",
        "  users_test_label_S.append(sum([y_test[i] for i in indices])/len(indices))\n",
        "  users_y_pred_S.append(sum([y_pred_knn[i] for i in indices])/len(indices))\n",
        "\n",
        "# Discretizar los valores continuos\n",
        "users_test_label_S_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_S]\n",
        "users_y_pred_S_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_S]\n",
        "\n",
        "print(len(users_test_label_S_discrete))\n",
        "print(len(users_y_pred_S_discrete))\n",
        "\n",
        "# Calcular la matriz de confusi√≥n\n",
        "cm = confusion_matrix(users_test_label_S_discrete, users_y_pred_S_discrete)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix S_aurata')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "X_test = scaler.transform(XTEST)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "print(len(XTRAIN))\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.scatter(train_tsne[(labels_S == 0), 0], train_tsne[(labels_S == 0), 1],\n",
        "            c='blue', marker='o', label='Labrax S')\n",
        "\n",
        "plt.scatter(train_tsne[(labels_S == 1), 0], train_tsne[(labels_S == 1), 1],\n",
        "            c='green', marker='o', label='Labrax C')\n",
        "\n",
        "plt.title(\"K-means Clusters in the t-SNE space\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = [\n",
        "    'red',      # Rojo\n",
        "    'blue',     # Azul\n",
        "    'green',    # Verde\n",
        "    'orange',   # Naranja\n",
        "    'purple',   # P√∫rpura\n",
        "    'brown',    # Marr√≥n\n",
        "    \n",
        "    'pink',     # Rosa\n",
        "    'gray',     # Gris\n",
        "    'olive',    # Oliva\n",
        "    'cyan',     # Cian\n",
        "    'gold',     # Dorado\n",
        "    'teal'      # Verde azulado\n",
        "]\n",
        "p_colors=[]\n",
        "\n",
        "for punto in XTRAIN:\n",
        "    embIMG=torch.from_numpy(punto[:512])\n",
        "    embIMG_norm = embIMG / embIMG.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * embIMG_norm @ text_features_Sclusters_norm.T).softmax(dim=-1)\n",
        "    _ , indices = similarity.topk(6)\n",
        "    p_colors.append(colors[indices[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(train_tsne)):\n",
        "    plt.scatter(train_tsne[i, 0], train_tsne[i, 1], color=p_colors[i] ,alpha=0.5)\n",
        "\n",
        "# Calcular los centroides en el espacio t-SNE\n",
        "centroids_tsne = np.array([\n",
        "    train_tsne[(clusters == i)].mean(axis=0) for i in range(n_clusters)\n",
        "])\n",
        "\n",
        "# A√±adir los centroides\n",
        "plt.scatter(centroids_tsne[:, 0], centroids_tsne[:, 1], c='black', marker='x', s=100, label='Centroids')\n",
        "\n",
        "plt.title(\"K-means Clusters in the t-SNE space\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "color_for_user = ['yellow'] * len(XTRAIN)\n",
        "\n",
        "claves = list(dict_users_train_S.keys())\n",
        "sorted_keys = sorted(claves, key=lambda key: len(dict_users_train_S[key]), reverse=True)\n",
        "\n",
        "# Seleccionar las 5 claves m√°s grandes\n",
        "top_5_keys = sorted_keys[:5]\n",
        "print(top_5_keys)\n",
        "\n",
        "# Obtener las posiciones de esas claves en la lista original\n",
        "top_5_positions = [claves.index(key) for key in top_5_keys]\n",
        "print(claves.index('SA193'))\n",
        "\n",
        "print(top_5_positions)\n",
        "\n",
        "top_5_lengths = [len(dict_users_train_S[claves[pos]]) for pos in top_5_positions]\n",
        "print(top_5_lengths)\n",
        "\n",
        "if claves[93] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[93]]:\n",
        "        color_for_user[indice] = 'red'\n",
        "if claves[110] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[110]]:\n",
        "        color_for_user[indice] = 'blue'\n",
        "if claves[45] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[45]]:\n",
        "        color_for_user[indice] = 'green'\n",
        "if claves[16] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[16]]:\n",
        "        color_for_user[indice] = 'cyan'\n",
        "if claves[105] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[105]]:\n",
        "        color_for_user[indice] = 'purple'\n",
        "\n",
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(embeddings_S)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(train_tsne)):\n",
        "    plt.scatter(train_tsne[i, 0], train_tsne[i, 1], color=color_for_user[i] ,alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(class_weights)\n",
        "\n",
        "model.add(Dense(128, input_dim=X_train.shape[1],use_bias=True))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='loss',  # Monitorea la p√©rdida de entrenamiento (sin validaci√≥n)\n",
        "    mode='min',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='loss',\n",
        "    factor=0.1,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, callbacks=[early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Calcular pesos para clases desbalanceadas\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Crear el modelo de Linear Probe\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))  # Clasificador lineal\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks para el entrenamiento\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Entrenamiento Loss')\n",
        "#plt.plot(history.history['val_loss'], label='Validaci√≥n Loss')\n",
        "plt.title('P√©rdida durante el entrenamiento')\n",
        "plt.xlabel('√âpocas')\n",
        "plt.ylabel('P√©rdida')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Entrenamiento Accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validaci√≥n Accuracy')\n",
        "plt.title('Exactitud durante el entrenamiento')\n",
        "plt.xlabel('√âpocas')\n",
        "plt.ylabel('Exactitud')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluaci√≥n en el conjunto de prueba\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Predicciones en el conjunto de prueba\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# Matriz de confusi√≥n normal\n",
        "print(\"\\nMatriz de confusi√≥n (General):\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix (General)\")\n",
        "plt.show()\n",
        "\n",
        "# Reporte de clasificaci√≥n\n",
        "print(\"\\nReporte de clasificaci√≥n (General):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Evaluaci√≥n por individuo\n",
        "users_test_label_S = []\n",
        "users_y_pred_S = []\n",
        "\n",
        "for _, indices in dict_users_S.items():\n",
        "    # Calcular promedio de etiquetas reales y predicciones por usuario\n",
        "    users_test_label_S.append(np.mean([y_test[i] for i in indices]))\n",
        "    users_y_pred_S.append(np.mean([y_pred[i] for i in indices]))\n",
        "\n",
        "# Discretizar valores continuos\n",
        "users_test_label_S_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_S]\n",
        "users_y_pred_S_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_S]\n",
        "\n",
        "# Verificar tama√±os consistentes\n",
        "assert len(users_test_label_S_discrete) == len(users_y_pred_S_discrete), \"Error: Los tama√±os no coinciden.\"\n",
        "\n",
        "# Matriz de confusi√≥n por individuo\n",
        "print(\"\\nMatriz de confusi√≥n (Por usuario):\")\n",
        "cm_users = confusion_matrix(users_test_label_S_discrete, users_y_pred_S_discrete)\n",
        "sns.heatmap(cm_users, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (User Level)')\n",
        "plt.show()\n",
        "\n",
        "# M√©tricas a nivel de usuario\n",
        "accuracy_users = accuracy_score(users_test_label_S_discrete, users_y_pred_S_discrete)\n",
        "print(f\"\\nAccuracy por usuario: {accuracy_users:.2f}\")\n",
        "print(\"\\nReporte de clasificaci√≥n (Por usuario):\")\n",
        "print(classification_report(users_test_label_S_discrete, users_y_pred_S_discrete))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "XEscap_S_scaled = scaler.transform(XEscap_S)\n",
        "y_escapS = model.predict(XEscap_S_scaled)\n",
        "y_escapS = (y_escapS > 0.5).astype(int).flatten()\n",
        "count_0, count_1 = np.bincount(y_escapS)\n",
        "\n",
        "print(\"Cautivos: \"+str(count_0))\n",
        "print(\"salvajes: \"+str(count_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "users_y_pred_ES_S = []\n",
        "for _ , indices in dict_users_ES_S.items():\n",
        "    users_y_pred_ES_S.append(np.mean([y_escapS[i] for i in indices]))\n",
        "users_y_pred_ES_S_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_ES_S]\n",
        "count_0, count_1 = np.bincount(users_y_pred_ES_S_discrete)\n",
        "\n",
        "print(\"Usuarios cautivos: \"+str(count_0))\n",
        "print(\"Usuarios salvajes: \"+str(count_1))\n",
        "\n",
        "ALL_embeddings_S = np.concatenate((XTRAIN, XEscap_S), axis=0)\n",
        "\n",
        "labels_ES_Slabels_S = np.concatenate((np.zeros(len(image_features_SS)),  # Aurata S (im√°genes)\n",
        "                         np.ones(len(image_features_SC)),   # Aurata C (im√°genes)\n",
        "                         np.ones(len(XEscap_S))*2))  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "embeddings_S_tsne = tsne.fit_transform(ALL_embeddings_S)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_S_tsne[(labels_ES_Slabels_S == 0), 0], embeddings_S_tsne[(labels_ES_Slabels_S == 0), 1],\n",
        "            c='blue', marker='o', label='Aurata S')\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_S_tsne[(labels_ES_Slabels_S == 1), 0], embeddings_S_tsne[(labels_ES_Slabels_S == 1), 1],\n",
        "            c='green', marker='o', label='Aurata C')\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_S_tsne[(labels_ES_Slabels_S == 2), 0], embeddings_S_tsne[(labels_ES_Slabels_S == 2), 1],\n",
        "            c='red', marker='s', label='Escapados')\n",
        "\n",
        "\n",
        "plt.title(\"t-SNE de los embeddings en 2D\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# D_LABRAX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "XTRAIN=embedings_DText\n",
        "XTEST=embeddings_DText_test\n",
        "labels_D = np.concatenate((np.zeros(len(image_features_DS)),  # Aurata S (im√°genes)\n",
        "                         np.ones(len(image_features_DC))), axis=0)  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "yTRAIN=labels_D\n",
        "yTEST=test_label_D\n",
        "\n",
        "XTRAIN = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in embedings_DText]\n",
        "XTEST = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in embeddings_DText_test]\n",
        "\n",
        "print(np.array(XTRAIN).shape)\n",
        "\n",
        "yTRAIN = np.array(yTRAIN)\n",
        "yTEST = np.array(yTEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "X_test = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_neighbors': [5, 7, 10, 15, 20],  # N√∫mero de vecinos\n",
        "    'weights': ['uniform', 'distance'],  # Peso de los vecinos\n",
        "    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']  # M√©tricas de distancia\n",
        "}\n",
        "\n",
        "grid_search_knn = GridSearchCV(\n",
        "    estimator=KNeighborsClassifier(),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # Validaci√≥n cruzada\n",
        "    scoring='accuracy',  # M√©trica para evaluar\n",
        "    verbose=1,\n",
        "    n_jobs=-1  # Paralelizar la b√∫squeda\n",
        ")\n",
        "\n",
        "# Ajustar el modelo a los datos de entrenamiento\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Mejores par√°metros para KNN:\", grid_search_knn.best_params_)\n",
        "\n",
        "best_knn = grid_search_knn.best_estimator_\n",
        "y_pred_knn = best_knn.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy con el mejor KNN: {accuracy_score(y_test, y_pred_knn):.2f}\")\n",
        "print(\"Reporte de clasificaci√≥n para KNN:\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "# Aplicar t-SNE solo a los datos de entrenamiento\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "indices = np.arange(len(y_test))\n",
        "correct = (y_pred_knn == y_test)\n",
        "\n",
        "all_embeddings = np.vstack((X_train, X_test))\n",
        "all_labels = np.concatenate((y_train, y_test))\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "all_tsne = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "train_tsne = all_tsne[:len(X_train)]\n",
        "test_tsne = all_tsne[len(X_train):]\n",
        "\n",
        "# Graficar en el mismo espacio\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 0), 0], train_tsne[np.where(y_train == 0), 1],\n",
        "    c='blue', marker='o', label='Labrax C (Train)', alpha=0.5\n",
        ")\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 1), 0], train_tsne[np.where(y_train == 1), 1],\n",
        "    c='green', marker='o', label='Labrax S (Train)', alpha=0.5\n",
        ")\n",
        "\n",
        "plt.scatter(\n",
        "    test_tsne[correct, 0], test_tsne[correct, 1],\n",
        "    c='black', marker='x', label='Predicci√≥n correcta (Test)', alpha=0.8\n",
        ")\n",
        "plt.scatter(\n",
        "    test_tsne[~correct, 0], test_tsne[~correct, 1],\n",
        "    c='red', marker='x', label='Predicci√≥n incorrecta (Test)', alpha=0.8\n",
        ")\n",
        "plt.title(\"t-SNE combinado: Entrenamiento y prueba\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "users_test_label_D=[]\n",
        "users_y_pred_D=[]\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "  users_test_label_D.append(sum([y_test[i] for i in indices])/len(indices))\n",
        "  users_y_pred_D.append(sum([y_pred_knn[i] for i in indices])/len(indices))\n",
        "\n",
        "# Discretizar los valores continuos\n",
        "users_test_label_D_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_D]\n",
        "users_y_pred_D_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_D]\n",
        "\n",
        "print(len(users_test_label_D_discrete))\n",
        "print(len(users_y_pred_D_discrete))\n",
        "\n",
        "# Calcular la matriz de confusi√≥n\n",
        "cm = confusion_matrix(users_test_label_D_discrete, users_y_pred_D_discrete)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix D_Labrax')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Escalar los embeddings\n",
        "scaler = StandardScaler()\n",
        "embeddings_D = scaler.fit_transform(XTRAIN)\n",
        "\n",
        "# Aplicar k-means con 3 clusters\n",
        "n_clusters = 2\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(embeddings_D)\n",
        "\n",
        "# Evaluar el resultado de k-means\n",
        "ari = adjusted_rand_score(yTRAIN, clusters)\n",
        "silhouette = silhouette_score(embeddings_D, clusters)\n",
        "\n",
        "print(f\"Adjusted Rand Index (ARI): {ari:.2f}\")\n",
        "print(f\"Silhouette Score: {silhouette:.2f}\")\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(embeddings_D)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(n_clusters):\n",
        "    plt.scatter(train_tsne[(clusters == i), 0], train_tsne[(clusters == i), 1],\n",
        "                label=f'Cluster {i + 1}', alpha=0.5)\n",
        "\n",
        "# Calcular los centroides en el espacio t-SNE\n",
        "centroids_tsne = np.array([\n",
        "    train_tsne[(clusters == i)].mean(axis=0) for i in range(n_clusters)\n",
        "])\n",
        "\n",
        "# A√±adir los centroides\n",
        "plt.scatter(centroids_tsne[:, 0], centroids_tsne[:, 1], c='black', marker='x', s=100, label='Centroides')\n",
        "\n",
        "plt.title(\"K-means Clusters in the t-SNE space\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Matriz de confusi√≥n entre clusters y etiquetas reales\n",
        "print(\"Matriz de confusi√≥n:\")\n",
        "print(confusion_matrix(yTRAIN, clusters))\n",
        "\n",
        "# Selecci√≥n de m√∫ltiples textos representativos por cl√∫ster\n",
        "num_representatives = 3  # N√∫mero de textos por cl√∫ster\n",
        "textos_representativos_por_cluster = []\n",
        "XTRAIN_array = np.array([np.concatenate((emb.numpy(), text.numpy())) for emb, text in embedings_DText])\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    # Calcular las distancias al centroide del cl√∫ster\n",
        "    distances = pairwise_distances([kmeans.cluster_centers_[i]], XTRAIN_array)\n",
        "    \n",
        "    # Obtener los √≠ndices de los puntos m√°s cercanos al centroide\n",
        "    closest_points_idx = np.argsort(distances[0])\n",
        "    selected_text_indices = set()  # Para evitar seleccionar el mismo texto m√°s de una vez\n",
        "    \n",
        "    # Lista temporal para los textos representativos del cl√∫ster actual\n",
        "    textos_representativos = []\n",
        "    for idx in closest_points_idx:\n",
        "        if len(textos_representativos) >= num_representatives:\n",
        "            break\n",
        "        \n",
        "        # Recuperar el embedding de texto m√°s cercano\n",
        "        _, closest_text_embedding = embedings_DText[idx]\n",
        "\n",
        "        # Convertir embedding a texto original buscando el √≠ndice correspondiente en `text_features_S`\n",
        "        similarity = (closest_text_embedding @ text_norm_D.T).softmax(dim=-1)\n",
        "        text_idx = similarity.argmax().item()  # √çndice del texto m√°s similar\n",
        "        \n",
        "        # Asegurar que el √≠ndice de texto no haya sido seleccionado antes\n",
        "        if text_idx not in selected_text_indices:\n",
        "            texto_representativo = text_labels_D[text_idx]  # Mapea al texto original\n",
        "            textos_representativos.append(texto_representativo)\n",
        "            selected_text_indices.add(text_idx)  # Marcar este √≠ndice como utilizado\n",
        "    \n",
        "    # A√±adir los textos representativos del cl√∫ster a la lista principal\n",
        "    textos_representativos_por_cluster.append(textos_representativos)\n",
        "\n",
        "# Mostrar los textos representativos por cl√∫ster\n",
        "print(\"Textos representativos por cl√∫ster:\")\n",
        "for i, textos in enumerate(textos_representativos_por_cluster, 1):\n",
        "    print(f\"Cl√∫ster {i}:\")\n",
        "    for texto in textos:\n",
        "        print(f\"  - {texto}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = [\n",
        "    'red',      # Rojo\n",
        "    'blue',     # Azul\n",
        "    'green',    # Verde\n",
        "    'orange',   # Naranja\n",
        "    'purple',   # P√∫rpura\n",
        "    'brown',    # Marr√≥n\n",
        "    \n",
        "    'pink',     # Rosa\n",
        "    'gray',     # Gris\n",
        "    'olive',    # Oliva\n",
        "    'cyan',     # Cian\n",
        "    'gold',     # Dorado\n",
        "    'teal'      # Verde azulado\n",
        "]\n",
        "p_colors=[]\n",
        "\n",
        "for punto in XTRAIN:\n",
        "    embIMG=torch.from_numpy(punto[:512])\n",
        "    embIMG_norm = embIMG / embIMG.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * embIMG_norm @ text_features_Dclusters_norm.T).softmax(dim=-1)\n",
        "    _ , indices = similarity.topk(6)\n",
        "    p_colors.append(colors[indices[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_train=scaler.fit_transform(XTRAIN)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(train_tsne)):\n",
        "    plt.scatter(train_tsne[i, 0], train_tsne[i, 1], color=p_colors[i] ,alpha=0.5)\n",
        "\n",
        "# Calcular los centroides en el espacio t-SNE\n",
        "centroids_tsne = np.array([\n",
        "    train_tsne[(clusters == i)].mean(axis=0) for i in range(n_clusters)\n",
        "])\n",
        "\n",
        "# A√±adir los centroides\n",
        "plt.scatter(centroids_tsne[:, 0], centroids_tsne[:, 1], c='black', marker='x', s=100, label='Centroids')\n",
        "\n",
        "plt.title(\"K-means Clusters in the t-SNE space\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = [\n",
        "    'blue',     # Rosa\n",
        "    'blue',     # Gris\n",
        "    'blue',    # Oliva\n",
        "    'blue',     # Cian\n",
        "    'blue',     # Dorado\n",
        "    'blue',      # Verde azulado\n",
        "    'red',      # Rojo\n",
        "    'red',     # Azul\n",
        "    'red',    # Verde\n",
        "    'red',   # Naranja\n",
        "    'red',   # P√∫rpura\n",
        "    'red',    # Marr√≥n\n",
        "]\n",
        "p_colors=[]\n",
        "\n",
        "for punto in XTRAIN:\n",
        "    embIMG=torch.from_numpy(punto[:512])\n",
        "    embIMG_norm = embIMG / embIMG.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * embIMG_norm @ text_features_Dclusters_norm.T).softmax(dim=-1)\n",
        "    _ , indices = similarity.topk(12)\n",
        "    p_colors.append(colors[indices[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(embeddings_D)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(train_tsne)):\n",
        "    plt.scatter(train_tsne[i, 0], train_tsne[i, 1], color=p_colors[i] ,alpha=0.5)\n",
        "\n",
        "# Calcular los centroides en el espacio t-SNE\n",
        "centroids_tsne = np.array([\n",
        "    train_tsne[(clusters == i)].mean(axis=0) for i in range(n_clusters)\n",
        "])\n",
        "\n",
        "# A√±adir los centroides\n",
        "plt.scatter(centroids_tsne[:, 0], centroids_tsne[:, 1], c='black', marker='x', s=100, label='Centroids')\n",
        "\n",
        "plt.title(\"K-means Clusters in the t-SNE space\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "X_test = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "# Definir grid de hiperpar√°metros para Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # Validaci√≥n cruzada\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1  # Paralelizaci√≥n\n",
        ")\n",
        "\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Mejores par√°metros para Random Forest:\", grid_search_rf.best_params_)\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy con Random Forest: {accuracy_score(y_test, y_pred_rf):.2f}\")\n",
        "print(\"Reporte de clasificaci√≥n para Random Forest:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "all_embeddings = np.vstack((X_train, X_test))\n",
        "all_labels = np.concatenate((y_train, y_test))\n",
        "all_tsne = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "train_tsne = all_tsne[:len(X_train)]\n",
        "test_tsne = all_tsne[len(X_train):]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 0), 0], train_tsne[np.where(y_train == 0), 1],\n",
        "    c='blue', marker='o', label='Aurata S (Train)', alpha=0.5\n",
        ")\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 1), 0], train_tsne[np.where(y_train == 1), 1],\n",
        "    c='green', marker='o', label='Aurata C (Train)', alpha=0.5\n",
        ")\n",
        "\n",
        "correct = (y_pred_rf == y_test)\n",
        "plt.scatter(\n",
        "    test_tsne[correct, 0], test_tsne[correct, 1],\n",
        "    c='black', marker='x', label='Predicci√≥n correcta (Test)', alpha=0.8\n",
        ")\n",
        "plt.scatter(\n",
        "    test_tsne[~correct, 0], test_tsne[~correct, 1],\n",
        "    c='red', marker='x', label='Predicci√≥n incorrecta (Test)', alpha=0.8\n",
        ")\n",
        "plt.title(\"t-SNE combinado: Entrenamiento y prueba\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "users_test_label_D=[]\n",
        "users_y_pred_D=[]\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "  users_test_label_D.append(sum([y_test[i] for i in indices])/len(indices))\n",
        "  users_y_pred_D.append(sum([y_pred_knn[i] for i in indices])/len(indices))\n",
        "\n",
        "# Discretizar los valores continuos\n",
        "users_test_label_D_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_D]\n",
        "users_y_pred_D_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_D]\n",
        "\n",
        "print(len(users_test_label_D_discrete))\n",
        "print(len(users_y_pred_D_discrete))\n",
        "\n",
        "# Calcular la matriz de confusi√≥n\n",
        "cm = confusion_matrix(users_test_label_D_discrete, users_y_pred_D_discrete)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix D_Labrax')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(class_weights)\n",
        "\n",
        "model.add(Dense(256, input_dim=X_train.shape[1],use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='loss',  # Monitorea la p√©rdida de entrenamiento (sin validaci√≥n)\n",
        "    mode='min',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='loss',\n",
        "    factor=0.1,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, callbacks=[early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Calcular pesos para clases desbalanceadas\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Crear el modelo de Linear Probe\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))  # Clasificador lineal\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks para el entrenamiento\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Entrenamiento Loss')\n",
        "#plt.plot(history.history['val_loss'], label='Validaci√≥n Loss')\n",
        "plt.title('P√©rdida durante el entrenamiento')\n",
        "plt.xlabel('√âpocas')\n",
        "plt.ylabel('P√©rdida')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Entrenamiento Accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validaci√≥n Accuracy')\n",
        "plt.title('Exactitud durante el entrenamiento')\n",
        "plt.xlabel('√âpocas')\n",
        "plt.ylabel('Exactitud')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluaci√≥n en el conjunto de prueba\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Predicciones en el conjunto de prueba\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# Matriz de confusi√≥n normal\n",
        "print(\"\\nMatriz de confusi√≥n (General):\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix (General)\")\n",
        "plt.show()\n",
        "\n",
        "# Reporte de clasificaci√≥n\n",
        "print(\"\\nReporte de clasificaci√≥n (General):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Evaluaci√≥n por individuo\n",
        "users_test_label_D = []\n",
        "users_y_pred_D = []\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "    # Calcular promedio de etiquetas reales y predicciones por usuario\n",
        "    users_test_label_D.append(np.mean([y_test[i] for i in indices]))\n",
        "    users_y_pred_D.append(np.mean([y_pred[i] for i in indices]))\n",
        "\n",
        "# Discretizar valores continuos\n",
        "users_test_label_D_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_D]\n",
        "users_y_pred_D_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_D]\n",
        "\n",
        "# Verificar tama√±os consistentes\n",
        "assert len(users_test_label_D_discrete) == len(users_y_pred_D_discrete), \"Error: Los tama√±os no coinciden.\"\n",
        "\n",
        "# Matriz de confusi√≥n por individuo\n",
        "print(\"\\nMatriz de confusi√≥n (Por usuario):\")\n",
        "cm_users = confusion_matrix(users_test_label_D_discrete, users_y_pred_D_discrete)\n",
        "sns.heatmap(cm_users, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (User Level)')\n",
        "plt.show()\n",
        "\n",
        "# M√©tricas a nivel de usuario\n",
        "accuracy_users = accuracy_score(users_test_label_D_discrete, users_y_pred_D_discrete)\n",
        "print(f\"\\nAccuracy por usuario: {accuracy_users:.2f}\")\n",
        "print(\"\\nReporte de clasificaci√≥n (Por usuario):\")\n",
        "print(classification_report(users_test_label_D_discrete, users_y_pred_D_discrete))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Escalado de los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "# Configuraci√≥n del GridSearchCV para SVC\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],  # Valores del par√°metro de penalizaci√≥n\n",
        "    'gamma': [0.01, 0.1, 1, 'scale', 'auto'],  # Valores del coeficiente del kernel\n",
        "    'kernel': ['linear', 'rbf', 'poly']  # Tipos de kernel\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=SVC(),  # Modelo SVC\n",
        "    param_grid=param_grid, \n",
        "    cv=5,  # Validaci√≥n cruzada con 5 particiones\n",
        "    verbose=1, \n",
        "    n_jobs=-1  # Paralelizaci√≥n\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "print(\"\\nMejores par√°metros encontrados:\")\n",
        "print(best_params)\n",
        "\n",
        "model_svc = grid_search.best_estimator_\n",
        "y_pred = model_svc.predict(X_test_scaled)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy del mejor modelo: {accuracy:.2f}\")\n",
        "print(\"\\nReporte de clasificaci√≥n:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "users_test_label_D=[]\n",
        "users_y_pred_D=[]\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "  users_test_label_D.append(sum([y_test[i] for i in indices])/len(indices))\n",
        "  users_y_pred_D.append(sum([y_pred_knn[i] for i in indices])/len(indices))\n",
        "\n",
        "# Discretizar los valores continuos\n",
        "users_test_label_D_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_D]\n",
        "users_y_pred_D_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_D]\n",
        "\n",
        "print(len(users_test_label_D_discrete))\n",
        "print(len(users_y_pred_D_discrete))\n",
        "\n",
        "# Calcular la matriz de confusi√≥n\n",
        "cm = confusion_matrix(users_test_label_D_discrete, users_y_pred_D_discrete)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix D_Labrax')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "XEscap_D_scaled = scaler.transform(XEscap_D)\n",
        "y_escapD = model.predict(XEscap_D_scaled)\n",
        "y_escapD = (y_escapD > 0.5).astype(int).flatten()\n",
        "count_0, count_1 = np.bincount(y_escapD)\n",
        "\n",
        "print(\"Cautivos: \"+str(count_0))\n",
        "print(\"salvajes: \"+str(count_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "users_y_pred_ES_D = []\n",
        "\n",
        "for _ , indices in dict_users_ES_D.items():\n",
        "    users_y_pred_ES_D.append(np.mean([y_escapD[i] for i in indices]))\n",
        "\n",
        "users_y_pred_ES_D_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_ES_D]\n",
        "count_0, count_1 = np.bincount(users_y_pred_ES_D_discrete)\n",
        "\n",
        "print(\"Usuarios cautivos: \"+str(count_0))\n",
        "print(\"Usuarios salvajes: \"+str(count_1))\n",
        "\n",
        "ALL_embeddings_D = np.concatenate((XTRAIN, XEscap_D), axis=0)\n",
        "\n",
        "labels_ES_Slabels_D = np.concatenate((np.zeros(len(image_features_DS)),  # Aurata S (im√°genes)\n",
        "                         np.ones(len(image_features_DC)),   # Aurata C (im√°genes)\n",
        "                         np.ones(len(XEscap_D))*2))  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "embeddings_D_tsne = tsne.fit_transform(ALL_embeddings_D)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_D_tsne[(labels_ES_Slabels_D == 0), 0], embeddings_D_tsne[(labels_ES_Slabels_D == 0), 1],\n",
        "            c='blue', marker='o', label='Labrax S')\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_D_tsne[(labels_ES_Slabels_D == 1), 0], embeddings_D_tsne[(labels_ES_Slabels_D == 1), 1],\n",
        "            c='green', marker='o', label='Labrax C')\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_D_tsne[(labels_ES_Slabels_D == 2), 0], embeddings_D_tsne[(labels_ES_Slabels_D == 2), 1],\n",
        "            c='red', marker='s', label='Escapados')\n",
        "\n",
        "\n",
        "plt.title(\"t-SNE de los embeddings en 2D\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gGsURoGALRtY",
        "7e1IhLruOtDv",
        "-x-3CzxFDIvy",
        "ZFS1aw-LPK3_",
        "5pbeQZG6OJJZ",
        "gTpeGjSFCV0Z"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
