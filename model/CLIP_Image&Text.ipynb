{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOAD DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbJecdT555QS",
        "outputId": "18118a60-0d43-4982-ba3b-c867f35ad552"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "test_rout=[]\n",
        "test_label=[]\n",
        "fish_type=[]\n",
        "carpetas = [r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\D_labrax\\test\\C\", r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\D_labrax\\test\\S\", r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\S_Aurata\\test\\C\",r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\S_Aurata\\test\\S\"]\n",
        "for carpeta in carpetas:\n",
        "  if carpeta[-1]=='C':\n",
        "    label=[0] * len(os.listdir(carpeta))\n",
        "  elif carpeta[-1]=='S':\n",
        "    label=[1] * len(os.listdir(carpeta))\n",
        "  else:\n",
        "    label=[2] * len(os.listdir(carpeta))\n",
        "  test_label.extend(label)\n",
        "  fish_type.extend(carpeta[-15]*len(os.listdir(carpeta)))\n",
        "  for archivo in os.listdir(carpeta):\n",
        "    if os.path.isfile(os.path.join(carpeta, archivo)):\n",
        "      test_rout.append(carpeta+'/'+archivo)\n",
        "\n",
        "len(test_rout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ad5INNee04q",
        "outputId": "8de2203e-6d85-46df-a3c9-92a166fa67a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train_rout=[]\n",
        "train_label=[]\n",
        "fish_type_train=[]\n",
        "\n",
        "carpetas = [r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\D_labrax\\train\\C\", r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\D_labrax\\train\\S\", r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\S_Aurata\\train\\C\",r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\S_Aurata\\train\\S\"]\n",
        "for carpeta in carpetas:\n",
        "  if carpeta[-1]=='C':\n",
        "    label=[0] * len(os.listdir(carpeta))\n",
        "  elif carpeta[-1]=='S':\n",
        "    label=[1] * len(os.listdir(carpeta))\n",
        "  else:\n",
        "    label=[2] * len(os.listdir(carpeta))\n",
        "  train_label.extend(label)\n",
        "  fish_type_train.extend(carpeta[-16]*len(os.listdir(carpeta)))\n",
        "  for archivo in os.listdir(carpeta):\n",
        "    if os.path.isfile(os.path.join(carpeta, archivo)):\n",
        "      train_rout.append(carpeta+'/'+archivo)\n",
        "\n",
        "len(train_rout)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "dict_users_train_S = {}\n",
        "filtered_ES_rout_S = [r for r, f in zip(train_rout, fish_type_train) if f == 'S']\n",
        "for idx, file_name in enumerate(filtered_ES_rout_S):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_train_S:\n",
        "        dict_users_train_S[nombre_base] = []\n",
        "    dict_users_train_S[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_train_S)\n",
        "\n",
        "dict_users_train_D = {}\n",
        "filtered_ES_rout_D = [r for r, f in zip(train_rout, fish_type_train) if f == 'D']\n",
        "for idx, file_name in enumerate(filtered_ES_rout_D):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_train_D:\n",
        "        dict_users_train_D[nombre_base] = []\n",
        "    dict_users_train_D[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_train_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "        print(f\"Usando GPU: {physical_devices}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configurando GPU: {e}\")\n",
        "else:\n",
        "    print(\"No se detectó GPU, usando CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktu25HWm-wBd",
        "outputId": "3e20a3b7-df5d-4c49-d5dd-1dd3ff810772"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "testES_rout=[]\n",
        "testES_label=[]\n",
        "fish_type_ES=[]\n",
        "\n",
        "carpetas = [r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\D_labrax\\E\",r\"D:\\GLORIA\\IJCNN\\Dataset_I_Seg\\S_Aurata\\E\"]\n",
        "for carpeta in carpetas:\n",
        "  if carpeta[-1]=='C':\n",
        "    label=[0] * len(os.listdir(carpeta))\n",
        "  elif carpeta[-1]=='S':\n",
        "    label=[1] * len(os.listdir(carpeta))\n",
        "  else:\n",
        "    label=[2] * len(os.listdir(carpeta))\n",
        "  testES_label.extend(label)\n",
        "  fish_type_ES.extend(carpeta[-10]*len(os.listdir(carpeta)))\n",
        "  for archivo in os.listdir(carpeta):\n",
        "    if os.path.isfile(os.path.join(carpeta, archivo)):\n",
        "      testES_rout.append(carpeta+'/'+archivo)\n",
        "\n",
        "dict_users_ES = {}\n",
        "\n",
        "for idx, file_name in enumerate(testES_rout):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_ES:\n",
        "        dict_users_ES[nombre_base] = []\n",
        "    dict_users_ES[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_ES)\n",
        "\n",
        "dict_users_ES_S = {}\n",
        "filtered_ES_rout_S = [r for r, f in zip(testES_rout, fish_type_ES) if f == 'S']\n",
        "for idx, file_name in enumerate(filtered_ES_rout_S):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_ES_S:\n",
        "        dict_users_ES_S[nombre_base] = []\n",
        "    dict_users_ES_S[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_ES_S)\n",
        "\n",
        "dict_users_ES_D = {}\n",
        "filtered_ES_rout_D = [r for r, f in zip(testES_rout, fish_type_ES) if f == 'D']\n",
        "for idx, file_name in enumerate(filtered_ES_rout_D):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_ES_D:\n",
        "        dict_users_ES_D[nombre_base] = []\n",
        "    dict_users_ES_D[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_ES_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gknw3OY6if-",
        "outputId": "32e993dd-ac9f-480f-811b-296ba26b43b8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "dict_users = {}\n",
        "\n",
        "for idx, file_name in enumerate(test_rout):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users:\n",
        "        dict_users[nombre_base] = []\n",
        "    dict_users[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "dict_users_S = {}\n",
        "filtered_test_rout_S = [r for r, f in zip(test_rout, fish_type) if f == 'S']\n",
        "for idx, file_name in enumerate(filtered_test_rout_S):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_S:\n",
        "        dict_users_S[nombre_base] = []\n",
        "    dict_users_S[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_S)\n",
        "\n",
        "dict_users_D = {}\n",
        "filtered_test_rout_D = [r for r, f in zip(test_rout, fish_type) if f == 'D']\n",
        "for idx, file_name in enumerate(filtered_test_rout_D):\n",
        "    nombre_archivo = os.path.basename(file_name)\n",
        "    nombre_base = re.sub(r'\\s?\\(\\d+\\)', '', nombre_archivo).split('_')[0]\n",
        "\n",
        "    if nombre_base not in dict_users_D:\n",
        "        dict_users_D[nombre_base] = []\n",
        "    dict_users_D[nombre_base].append(idx)\n",
        "\n",
        "print(dict_users_D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLIP LUBINA (D_LABRAX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "results=[]\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "test_label_D=[]\n",
        "imagens_tensor_D = []\n",
        "for r, fish, l in zip(test_rout, fish_type, test_label):\n",
        "  if fish == \"D\":\n",
        "    image = Image.open(r)\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "    imagens_tensor_D.append(image_input)\n",
        "    test_label_D.append(l)\n",
        "\n",
        "# 84%\n",
        "'''\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side and an oval body and in which the mouth slightly tilted upwards,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, a color that transitions from a darker upper side, to a white ventral side and in which the mouth follows the same direction as the main axis of the body\", # Wild\n",
        "]\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side and an oval body and in which the mouth slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, a color that transitions from a darker upper side to a white ventral side and in which the mouth follows the same direction as the main axis of the body\", # Wild\n",
        "]\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side and an oval body and in which the mouth slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, a color that transitions from a darker upper side to a white ventral side and in which the mouth follows the same direction as the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "'''\n",
        "# 85%\n",
        "'''\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side and an oval body and in which the mouth slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, the color transitions from a darker upper side to a white ventral side and in which the mouth follows direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "'''\n",
        "\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side, an oval body and the mouth is slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, the color transitions from a darker upper side to a white ventral side and in which the mouth follows direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "classes = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side, an oval body and the mouth is slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, the color transitions from a darker upper side to a white ventral side and in which the mouth follows direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "labels = [\"Farm fish\", \"Wild fish\"]\n",
        "\n",
        "\n",
        "text_inputs = torch.cat([clip.tokenize(c) for c in classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "  text_features = model.encode_text(text_inputs)\n",
        "\n",
        "batch_images = torch.cat(imagens_tensor_D, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_features_D = model.encode_image(batch_images)\n",
        "\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "for imgF in image_features_D:\n",
        "  imgF_norm = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * imgF_norm @ text_features.T).softmax(dim=-1)\n",
        "  values, indices = similarity.topk(2)\n",
        "  results.append([indices.tolist()[0],values.tolist()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred= [fila[0] for fila in results]\n",
        "print(\"\\nReporte de clasificación:\\n\", classification_report(test_label_D, y_pred))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(test_label_D, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "users_test_label_D=[]\n",
        "users_y_pred_D=[]\n",
        "user_y_prob_D=[]\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "  users_test_label_D.append(round(sum([test_label_D[i] for i in indices])/len(indices)))\n",
        "  prob=[]\n",
        "  for z in [results[i] for i in indices]:\n",
        "    if z[0]==1:\n",
        "      prob.append(z[1])\n",
        "    else:\n",
        "      prob.append(1-z[1])\n",
        "  users_y_pred_D.append(round(sum(prob)/len(indices)))\n",
        "  user_y_prob_D.append((sum(prob)/len(indices)))\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nReporte de clasificación:\\n\", classification_report(users_test_label_D, users_y_pred_D))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(users_test_label_D, users_y_pred_D)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLIP DORADA (S_AURATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LUBINA\n",
        "\n",
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "results=[]\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "test_label_S=[]\n",
        "imagens_tensor_S = []\n",
        "for r, fish, l in zip(test_rout, fish_type, test_label):\n",
        "  if fish == \"S\":\n",
        "    image = Image.open(r)\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "    imagens_tensor_S.append(image_input)\n",
        "    test_label_S.append(l)\n",
        "\n",
        "# 80%\n",
        "'''\n",
        "classes = [\n",
        "    \"a close-up of a dark grey fish with a curved bottom, the lateral fins are close to the body and small\",  # Captivity\n",
        "    \"a close-up of a fish with a flat bottom and in mouth follows the direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "'''\n",
        "'''\n",
        "# 84 %\n",
        "classes = [\n",
        "    \"a close-up of a dark grey fish with a curved bottom, the lateral fins are close to the body and small\",  # Captivity\n",
        "    \"a close-up of a fish with a flat bottom and in mouth follows the direction of the main axis of the body,the lateral fins separated from the body and oriented slightly backward and the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "'''\n",
        "classes = [\n",
        "    \"a close-up of a dark grey fish with an oval body, a white ventral side with high contrast with the body, the mouth is slightly tilted upwards, the lateral fins are close to the body and small\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, a white ventral side with high contrast with the body, the mouth follows the direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "classes = [\n",
        "    \"a close-up of a dark grey fish with a curved bottom, the lateral fins are close to the body and small\",  # Captivity\n",
        "    \"a close-up of a fish with a flat bottom and in mouth follows the direction of the main axis of the body, the lateral fins separated from the body and oriented slightly backward and the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "labels=[\"Farmed fish\",\"Wild fish\"]\n",
        "\n",
        "text_inputs = torch.cat([clip.tokenize(c) for c in classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "  text_features = model.encode_text(text_inputs)\n",
        "\n",
        "batch_images = torch.cat(imagens_tensor_S, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_features_S = model.encode_image(batch_images)\n",
        "\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "for imgF in image_features_S:\n",
        "  imgF_norm = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * imgF_norm @ text_features.T).softmax(dim=-1)\n",
        "  values, indices = similarity.topk(2)\n",
        "  results.append([indices.tolist()[0],values.tolist()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred= [fila[0] for fila in results]\n",
        "print(\"\\nReporte de clasificación:\\n\", classification_report(test_label_S, y_pred))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(test_label_S, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "users_test_label_S=[]\n",
        "users_y_pred_S=[]\n",
        "user_y_prob_S=[]\n",
        "\n",
        "for _, indices in dict_users_S.items():\n",
        "  users_test_label_S.append(round(sum([test_label_S[i] for i in indices])/len(indices)))\n",
        "  prob=[]\n",
        "  for z in [results[i] for i in indices]:\n",
        "    if z[0]==1:\n",
        "      prob.append(z[1])\n",
        "    else:\n",
        "      prob.append(1-z[1])\n",
        "  users_y_pred_S.append(round(sum(prob)/len(indices)))\n",
        "  user_y_prob_S.append((sum(prob)/len(indices)))\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nReporte de clasificación:\\n\", classification_report(users_test_label_S, users_y_pred_S))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(users_test_label_S, users_y_pred_S)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFS1aw-LPK3_"
      },
      "source": [
        "## Para cada especie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di6nE62MPPB9",
        "outputId": "b9e7e930-2107-4015-d227-3255b98c4c7e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "labels=[\"Fish in captivity\",\"Wild fish\"]\n",
        "\n",
        "device = \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "imagens_tensor_SC = []\n",
        "imagens_tensor_DC = []\n",
        "\n",
        "train_rout_C = [rout for rout, l in zip(train_rout, train_label) if l == 0]\n",
        "fish_type_train_C = [fish for fish, l in zip(fish_type_train, train_label) if l == 0]\n",
        "\n",
        "for r, fish_t in zip(train_rout_C, fish_type_train_C):\n",
        "  image = Image.open(r)\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  if fish_t == 'S':\n",
        "    imagens_tensor_SC.append(image_input)\n",
        "  elif fish_t == 'D':\n",
        "    imagens_tensor_DC.append(image_input)\n",
        "\n",
        "batch_images_SC = torch.cat(imagens_tensor_SC, dim=0)\n",
        "batch_images_DC = torch.cat(imagens_tensor_DC, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_features_SC = model.encode_image(batch_images_SC)\n",
        "  image_features_DC = model.encode_image(batch_images_DC)\n",
        "\n",
        "imagens_tensor_SS = []\n",
        "imagens_tensor_DS = []\n",
        "\n",
        "train_rout_S = [rout for rout, l in zip(train_rout, train_label) if l == 1]\n",
        "fish_type_train_S = [fish for fish, l in zip(fish_type_train, train_label) if l == 1]\n",
        "\n",
        "for r, fish_t in zip(train_rout_S, fish_type_train_S):\n",
        "  image = Image.open(r)\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  if fish_t == 'S':\n",
        "    imagens_tensor_SS.append(image_input)\n",
        "  elif fish_t == 'D':\n",
        "    imagens_tensor_DS.append(image_input)\n",
        "\n",
        "batch_images_SS = torch.cat(imagens_tensor_SS, dim=0)\n",
        "batch_images_DS = torch.cat(imagens_tensor_DS, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_features_SS = model.encode_image(batch_images_SS)\n",
        "  image_features_DS = model.encode_image(batch_images_DS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imagens_tensor_ES=[]\n",
        "imagens_tensor_ED=[]\n",
        "\n",
        "device = \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "for fish_t, r in zip(fish_type_ES,testES_rout):\n",
        "  image = Image.open(r)\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  if fish_t == 'S':\n",
        "    imagens_tensor_ES.append(image_input)\n",
        "  elif fish_t == 'D':\n",
        "    imagens_tensor_ED.append(image_input)\n",
        "batch_images_ED = torch.cat(imagens_tensor_ED, dim=0)\n",
        "batch_images_ES = torch.cat(imagens_tensor_ES, dim=0)\n",
        "with torch.no_grad():\n",
        "  image_features_ES = model.encode_image(batch_images_ES)\n",
        "  image_features_ED = model.encode_image(batch_images_ED)\n",
        "\n",
        "text_labels_S = [\n",
        "    \"a close-up of a dark grey fish with a curved bottom, the lateral fins are close to the body and small\",  # Captivity\n",
        "    \"a close-up of a fish with a flat bottom and in mouth follows the direction of the main axis of the body, the lateral fins separated from the body and oriented slightly backward and the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_S]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_S = model.encode_text(text_inputs)\n",
        "\n",
        "text_features_S_norm = text_features_S / text_features_S.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_labels_D = [\n",
        "    \"a close-up of a fish with a uniform gray color that transitions to a white ventral side, an oval body and the mouth is slightly tilted upwards, the lateral fins are close to the body and small,\",  # Captivity\n",
        "    \"a close-up of a fish with a straight body, the color transitions from a darker upper side to a white ventral side and in which the mouth follows direction of the main axis of the body, the dorsal fin is symmetrical\", # Wild\n",
        "]\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_D]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_D = model.encode_text(text_inputs)\n",
        "\n",
        "text_features_D_norm = text_features_D / text_features_D.norm(dim=-1, keepdim=True)\n",
        "\n",
        "all_features_S=[]\n",
        "for imgF in image_features_ES:\n",
        "    imgF2 = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "    similarity = 100.0 * imgF2 @ text_features_S_norm.T  \n",
        "    values, indices = similarity.topk(2)\n",
        "    all_features_S.append([imgF,text_features_S[indices[0]]])\n",
        "\n",
        "all_features_D=[]\n",
        "for imgF in image_features_ED:\n",
        "    imgF2 = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "    similarity = 100.0 * imgF2 @ text_features_D_norm.T  \n",
        "    values, indices = similarity.topk(2)\n",
        "    all_features_D.append([imgF,text_features_D[indices[0]]])\n",
        "\n",
        "XEscap_S = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in all_features_S]\n",
        "XEscap_D = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in all_features_D]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zChSStNITLHS"
      },
      "outputs": [],
      "source": [
        "comb_image_features_SC = torch.mean(image_features_SC, dim=0)\n",
        "comb_image_features_DC = torch.mean(image_features_DC, dim=0)\n",
        "comb_image_features_SS = torch.mean(image_features_SS, dim=0)\n",
        "comb_image_features_DS = torch.mean(image_features_DS, dim=0)\n",
        "\n",
        "comb_image_features_SC /= comb_image_features_SC.norm(dim=-1, keepdim=True)\n",
        "comb_image_features_DC /= comb_image_features_DC.norm(dim=-1, keepdim=True)\n",
        "comb_image_features_SS /= comb_image_features_SS.norm(dim=-1, keepdim=True)\n",
        "comb_image_features_DS /= comb_image_features_DS.norm(dim=-1, keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwOzjsoCTprA",
        "outputId": "32df9068-29fc-437a-f91a-dfcbdd816096"
      },
      "outputs": [],
      "source": [
        "alpha=0.5\n",
        "\n",
        "results_S=[]\n",
        "results_D=[]\n",
        "\n",
        "test_imagens_tensor_S = []\n",
        "test_imagens_tensor_D = []\n",
        "test_label_S=[]\n",
        "test_label_D=[]\n",
        "\n",
        "text_labels_S = [\n",
        "    \"A close-up of a fish where the lateral line is barely visible and fragmented\",\n",
        "    \"A close-up of a fish with a slightly curved mouth\",\n",
        "    \"A close-up of a fish with a light underside sharply contrasting its darker grey body.\",\n",
        "    \"A close-up of a fish characterized by a consistently structured and evident lateral line\",\n",
        "    \"A close-up of a fish with a mouth maintaining a level alignment\",\n",
        "    \"A close-up of a fish with a bright grey body transitioning to white underneath.\",\n",
        "]\n",
        "\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_S]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_S = model.encode_text(text_inputs)\n",
        "\n",
        "text_features_S_norm = text_features_S / text_features_S.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_labels_D = [\n",
        "    # Cultivada\n",
        "    \"A close-up of a fish with an even transition from snout to lips\",\n",
        "    \"A close-up of a fish where the lateral line is barely visible and fragmented\",\n",
        "    \"A close-up of a fish with a dorsal fin that is worn out.\",\n",
        "\n",
        "    # Salvaje\n",
        "    \"A close-up of a fish with a pronounced drop from forehead to lips\",\n",
        "    \"A close-up of a fish where the lateral line appears continuous and well-marked\",\n",
        "    \"A close-up of a fish with fins that are not close to the body\",\n",
        "]\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_D]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_D = model.encode_text(text_inputs)\n",
        "\n",
        "text_features_D_norm = text_features_D / text_features_D.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "for r, fish_t, lab_t in zip(test_rout, fish_type, test_label):\n",
        "  image = Image.open(r)\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  if fish_t == 'S':\n",
        "    test_imagens_tensor_S.append(image_input)\n",
        "    test_label_S.append(lab_t)\n",
        "  elif fish_t == 'D':\n",
        "    test_imagens_tensor_D.append(image_input)\n",
        "    test_label_D.append(lab_t)\n",
        "\n",
        "test_batch_images_S = torch.cat(test_imagens_tensor_S, dim=0)\n",
        "test_batch_images_D = torch.cat(test_imagens_tensor_D, dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  test_image_features_S = model.encode_image(test_batch_images_S)\n",
        "  test_image_features_D = model.encode_image(test_batch_images_D)\n",
        "\n",
        "for imgF in test_image_features_S:\n",
        "    imgF2 = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Similaridad con imágenes y texto\n",
        "    similarity_C_image = 100.0 * imgF2 @ comb_image_features_SC.T\n",
        "    similarity_S_image = 100.0 * imgF2 @ comb_image_features_SS.T\n",
        "\n",
        "    similarity_C_text = (100.0 * imgF2 @ text_features_S_norm[:3].T).mean(dim=-1)\n",
        "    similarity_S_text = (100.0 * imgF2 @ text_features_S_norm[3:].T).mean(dim=-1)  \n",
        "\n",
        "    similarity_C_combined = alpha * similarity_C_image + (1 - alpha) * similarity_C_text\n",
        "    similarity_S_combined = alpha * similarity_S_image + (1 - alpha) * similarity_S_text\n",
        "\n",
        "    # Combinar las similitudes (imágenes y texto)\n",
        "    similarity_combined = torch.cat(\n",
        "        (\n",
        "            similarity_C_combined.unsqueeze(0),\n",
        "            similarity_S_combined.unsqueeze(0)\n",
        "        ), \n",
        "        dim=-1\n",
        "    )\n",
        "\n",
        "    # Aplicar softmax para obtener probabilidades\n",
        "    similarity = similarity_combined.softmax(dim=-1)\n",
        "    values, indices = similarity.topk(2)\n",
        "    results_S.append([indices.tolist()[0], values.tolist()[0]])\n",
        "\n",
        "for imgF in test_image_features_D:\n",
        "    imgF2 = imgF / imgF.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Similaridad con imágenes y texto\n",
        "    similarity_C_image = 100.0 * imgF2 @ comb_image_features_DC.T\n",
        "    similarity_S_image = 100.0 * imgF2 @ comb_image_features_DS.T\n",
        "\n",
        "    similarity_C_text = (100.0 * imgF2 @ text_features_D_norm[:3].T).mean(dim=-1)\n",
        "    similarity_S_text = (100.0 * imgF2 @ text_features_D_norm[3:].T).mean(dim=-1)  \n",
        "\n",
        "    similarity_C_combined = alpha * similarity_C_image + (1 - alpha) * similarity_C_text\n",
        "    similarity_S_combined = alpha * similarity_S_image + (1 - alpha) * similarity_S_text\n",
        "\n",
        "    # Combinar las similitudes (imágenes y texto)\n",
        "    similarity_combined = torch.cat(\n",
        "        (\n",
        "            similarity_C_combined.unsqueeze(0),\n",
        "            similarity_S_combined.unsqueeze(0)\n",
        "        ), \n",
        "        dim=-1\n",
        "    )\n",
        "\n",
        "\n",
        "    # Aplicar softmax para obtener probabilidades\n",
        "    similarity = similarity_combined.softmax(dim=-1)\n",
        "    values, indices = similarity.topk(2)\n",
        "    results_D.append([indices.tolist()[0], values.tolist()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXtNkbpPXWUM",
        "outputId": "a8149e0f-dcf6-4b9f-e40a-6261dc99cdda"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred_S= [fila[0] for fila in results_S]\n",
        "test_label_S = [int(label) for label in test_label_S]\n",
        "assert len(y_pred_S) == len(test_label_S)\n",
        "print(\"\\nReporte de clasificación en S_Aurata:\\n\", classification_report(test_label_S, y_pred_S))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCXPyS1bXtzf",
        "outputId": "9003d341-2943-43cf-95fc-d07dc431f5ec"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred_D= [fila[0] for fila in results_D]\n",
        "print(\"\\nReporte de clasificación en D_labrax:\\n\", classification_report(test_label_D, y_pred_D))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALJeTwqKXis1"
      },
      "outputs": [],
      "source": [
        "users_test_label_S=[]\n",
        "users_y_pred_S=[]\n",
        "user_y_prob_S=[]\n",
        "\n",
        "for _, indices in dict_users_S.items():\n",
        "  users_test_label_S.append(sum([test_label_S[i] for i in indices])/len(indices))\n",
        "\n",
        "  prob=[]\n",
        "  for z in [results_S[i] for i in indices]:\n",
        "    if z[0]==1:\n",
        "      prob.append(z[1])\n",
        "    else:\n",
        "      prob.append(1-z[1])\n",
        "  users_y_pred_S.append(round(sum(prob)/len(indices)))\n",
        "  user_y_prob_S.append((sum(prob)/len(indices)))\n",
        "\n",
        "users_test_label_D=[]\n",
        "users_y_pred_D=[]\n",
        "user_y_prob_D=[]\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "  users_test_label_D.append(sum([test_label_D[i] for i in indices])/len(indices))\n",
        "\n",
        "  prob=[]\n",
        "  for z in [results_D[i] for i in indices]:\n",
        "    if z[0]==1:\n",
        "      prob.append(z[1])\n",
        "    else:\n",
        "      prob.append(1-z[1])\n",
        "  users_y_pred_D.append(round(sum(prob)/len(indices)))\n",
        "  user_y_prob_D.append((sum(prob)/len(indices)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK9aC8dsd3j4",
        "outputId": "3f70da93-2cf7-475b-a5fe-5d9a1b40532c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nReporte de clasificación:\\n\", classification_report(users_test_label_S, users_y_pred_S))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoQQS3KPd4D7",
        "outputId": "c1524a2e-74f8-471c-d686-d282ad5d46d3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nReporte de clasificación:\\n\", classification_report(users_test_label_D, users_y_pred_D))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "qXjO3AahdxFi",
        "outputId": "5d1f1388-3ac1-459c-b810-c49396def59a"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(users_test_label_S, users_y_pred_S)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix S_aurata')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "eGGYiB2reMcE",
        "outputId": "7cdd9816-9bce-4b0b-f5f8-35110354542a"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(users_test_label_D, users_y_pred_D)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix D_labrax')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T-SNE AURATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "5egfwoLyesM_",
        "outputId": "ccb5cc29-8ad5-4afd-8be7-bd03c2a3243e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "embeddings_S = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "text_features_S_extended = np.vstack((text_features_S[0].unsqueeze(0).numpy(),\n",
        "                                      text_features_S[1].unsqueeze(0).numpy()))\n",
        "embeddings_S_Text = np.concatenate((embeddings_S, text_features_S_extended), axis=0)\n",
        "embeddings_S_test = test_image_features_S\n",
        "\n",
        "labels_S = np.concatenate((np.zeros(len(image_features_SS)),  # Aurata S (imágenes)\n",
        "                         np.ones(len(image_features_SC)),   # Aurata C (imágenes)\n",
        "                         [2, 3]))  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "labels_test = test_label_S\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "embeddings_S_tsne = tsne.fit_transform(embeddings_S_Text)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Clase 0: Aurata S\n",
        "plt.scatter(embeddings_S_tsne[(labels_S == 0), 0], embeddings_S_tsne[(labels_S == 0), 1],\n",
        "            c='blue', marker='o', label='Aurata S')\n",
        "\n",
        "# Clase 1: Aurata C\n",
        "plt.scatter(embeddings_S_tsne[(labels_S == 1), 0], embeddings_S_tsne[(labels_S == 1), 1],\n",
        "            c='green', marker='o', label='Aurata C')\n",
        "\n",
        "# Clase 0: Aurata S\n",
        "plt.scatter(embeddings_S_tsne[(labels_S == 2), 0], embeddings_S_tsne[(labels_S == 2), 1],\n",
        "            c='red', marker='s', label='EmbTexto S')\n",
        "\n",
        "# Clase 1: Aurata C\n",
        "plt.scatter(embeddings_S_tsne[(labels_S == 3), 0], embeddings_S_tsne[(labels_S == 3), 1],\n",
        "            c='red', marker='s', label='EmbTexto C')\n",
        "\n",
        "plt.title(\"t-SNE de los embeddings en 2D\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import MDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "embeddings_S = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "text_features_S_extended = np.vstack((text_features_S[0].unsqueeze(0).numpy(),\n",
        "                                      text_features_S[1].unsqueeze(0).numpy()))\n",
        "embeddings_S_Text = np.concatenate((embeddings_S, text_features_S_extended), axis=0)\n",
        "\n",
        "labels_S = np.concatenate((\n",
        "    np.zeros(len(image_features_SS)),  # Clase 0: Aurata S\n",
        "    np.ones(len(image_features_SC)),   # Clase 1: Aurata C\n",
        "    [2, 3]  # Clase 2: Embedding Texto S, Clase 3: Embedding Texto C\n",
        "))\n",
        "\n",
        "# Aplicar MDS\n",
        "mds = MDS(n_components=2, random_state=42)\n",
        "embeddings_S_mds = mds.fit_transform(embeddings_S_Text)\n",
        "\n",
        "# Graficar MDS\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Clase 0: Aurata S\n",
        "plt.scatter(embeddings_S_mds[(labels_S == 0), 0], embeddings_S_mds[(labels_S == 0), 1],\n",
        "            c='blue', marker='o', label='Aurata S')\n",
        "\n",
        "# Clase 1: Aurata C\n",
        "plt.scatter(embeddings_S_mds[(labels_S == 1), 0], embeddings_S_mds[(labels_S == 1), 1],\n",
        "            c='green', marker='o', label='Aurata C')\n",
        "\n",
        "# Clase 2: Embedding Texto S\n",
        "plt.scatter(embeddings_S_mds[(labels_S == 2), 0], embeddings_S_mds[(labels_S == 2), 1],\n",
        "            c='red', marker='s', label='EmbTexto S')\n",
        "\n",
        "# Clase 3: Embedding Texto C\n",
        "plt.scatter(embeddings_S_mds[(labels_S == 3), 0], embeddings_S_mds[(labels_S == 3), 1],\n",
        "            c='orange', marker='s', label='EmbTexto C')\n",
        "\n",
        "plt.title(\"MDS de los embeddings en 2D\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "#text_features_D /= np.linalg.norm(text_features_D, axis=1, keepdims=True)\n",
        "\n",
        "embeddings_D = np.concatenate((image_features_DS, image_features_DC), axis=0)\n",
        "text_features_D_extended = np.vstack((text_features_D[0].unsqueeze(0).numpy(),\n",
        "                                      text_features_D[1].unsqueeze(0).numpy()))\n",
        "embeddings_D_Text = np.concatenate((embeddings_D, text_features_D_extended), axis=0)\n",
        "embeddings_D_test = test_image_features_D\n",
        "\n",
        "labels_D = np.concatenate((np.zeros(len(image_features_DS)),  # Aurata S (imágenes)\n",
        "                         np.ones(len(image_features_DC)),   # Aurata C (imágenes)\n",
        "                         [2, 3]))  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "labels_test = test_label_D\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "embeddings_D_tsne = tsne.fit_transform(embeddings_D_Text)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Clase 0: Aurata S\n",
        "plt.scatter(embeddings_D_tsne[(labels_D == 0), 0], embeddings_D_tsne[(labels_D == 0), 1],\n",
        "            c='blue', marker='o', label='Dorada S')\n",
        "\n",
        "# Clase 1: Aurata C\n",
        "plt.scatter(embeddings_D_tsne[(labels_D == 1), 0], embeddings_D_tsne[(labels_D == 1), 1],\n",
        "            c='green', marker='o', label='Doarada C')\n",
        "\n",
        "# Clase 0: Aurata S\n",
        "plt.scatter(embeddings_D_tsne[(labels_D == 2), 0], embeddings_D_tsne[(labels_D == 2), 1],\n",
        "            c='red', marker='s', label='EmbTexto S')\n",
        "\n",
        "# Clase 1: Aurata C\n",
        "plt.scatter(embeddings_D_tsne[(labels_D == 3), 0], embeddings_D_tsne[(labels_D == 3), 1],\n",
        "            c='red', marker='s', label='EmbTexto C')\n",
        "\n",
        "plt.title(\"t-SNE de los embeddings en 2D\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_labels_S_ref = [\n",
        "    \"a close-up of a cultivated fish\",  # Captivity\n",
        "    \"a close-up of a wild fish\", # Wild\n",
        "]\n",
        "text_labels_D_ref = [\n",
        "    \"a close-up of a cultivated fish\",  # Captivity\n",
        "    \"a close-up of a wild fish\", # Wild\n",
        "]\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_S_ref]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_S_ref = model.encode_text(text_inputs)\n",
        "\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_D_ref]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_D_ref = model.encode_text(text_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedings_DText=[]\n",
        "embeddings_DText_test=[]\n",
        "\n",
        "embeddings_D_tensor = torch.tensor(embeddings_D)\n",
        "embeddings_D_test_tensor = torch.tensor(embeddings_D_test)\n",
        "\n",
        "text_norm_D = text_features_D_norm\n",
        "\n",
        "for emb in embeddings_D_tensor:\n",
        "  emb_norm = emb / emb.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * emb_norm @ text_norm_D.T).softmax(dim=-1)\n",
        "  similarity_class_0 = similarity[:3].mean()  # Average similarity for class 0 (Cultivada)\n",
        "  similarity_class_1 = similarity[3:].mean()  # Average similarity for class 1 (Salvaje)\n",
        "  class_similarities = torch.tensor([similarity_class_0, similarity_class_1])\n",
        "  hola, indices = class_similarities.topk(1)\n",
        "  most_similar_text_embedding = text_features_D_ref[indices[0]]\n",
        "  embedings_DText.append((emb, most_similar_text_embedding))\n",
        "\n",
        "for emb in embeddings_D_test:\n",
        "  emb_norm = emb / emb.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * emb_norm @ text_norm_D.T).softmax(dim=-1)\n",
        "  similarity_class_0 = similarity[:3].mean()  # Average similarity for class 0 (Cultivada)\n",
        "  similarity_class_1 = similarity[3:].mean()  # Average similarity for class 1 (Salvaje)\n",
        "  class_similarities = torch.tensor([similarity_class_0, similarity_class_1])\n",
        "  _, indices = class_similarities.topk(1)\n",
        "  most_similar_text_embedding = text_features_D_ref[indices[0]]\n",
        "  embeddings_DText_test.append((emb, most_similar_text_embedding))\n",
        "\n",
        "embedings_SText = []\n",
        "embeddings_SText_test = []\n",
        "\n",
        "# Assuming `embeddings_S` and `embeddings_S_test` are defined tensors\n",
        "embeddings_S_tensor = torch.tensor(embeddings_S)\n",
        "embeddings_S_test_tensor = torch.tensor(embeddings_S_test)\n",
        "\n",
        "# Normalized text features\n",
        "text_norm_S = text_features_S_norm\n",
        "\n",
        "# Processing embeddings for training\n",
        "for emb in embeddings_S_tensor:\n",
        "    emb_norm = emb / emb.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * emb_norm @ text_norm_S.T).softmax(dim=-1)\n",
        "    similarity_class_0 = similarity[:3].mean()  # Average similarity for class 0 (Cultivada)\n",
        "    similarity_class_1 = similarity[3:].mean()  # Average similarity for class 1 (Salvaje)\n",
        "    class_similarities = torch.tensor([similarity_class_0, similarity_class_1])\n",
        "    _, indices = class_similarities.topk(1)\n",
        "    most_similar_text_embedding = text_features_S_ref[indices[0]]\n",
        "    embedings_SText.append((emb, most_similar_text_embedding))\n",
        "\n",
        "# Processing embeddings for testing\n",
        "for emb in embeddings_S_test_tensor:\n",
        "    emb_norm = emb / emb.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * emb_norm @ text_norm_S.T).softmax(dim=-1)\n",
        "    similarity_class_0 = similarity[:3].mean()  # Average similarity for class 0 (Cultivada)\n",
        "    similarity_class_1 = similarity[3:].mean()  # Average similarity for class 1 (Salvaje)\n",
        "    class_similarities = torch.tensor([similarity_class_0, similarity_class_1])\n",
        "    _, indices = class_similarities.topk(1)\n",
        "    most_similar_text_embedding = text_features_S_ref[indices[0]]\n",
        "    embeddings_SText_test.append((emb, most_similar_text_embedding))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# S_Aurata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "XTRAIN=embedings_SText\n",
        "XTEST=embeddings_SText_test\n",
        "labels_S = np.concatenate((np.zeros(len(image_features_SS)),  # Aurata S (imágenes)\n",
        "                         np.ones(len(image_features_SC))), axis=0)  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "yTRAIN=labels_S\n",
        "yTEST=test_label_S\n",
        "\n",
        "XTRAIN = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in embedings_SText]\n",
        "XTEST = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in embeddings_SText_test]\n",
        "\n",
        "print(len(XTRAIN))\n",
        "\n",
        "yTRAIN = np.array(yTRAIN)\n",
        "yTEST = np.array(yTEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "X_test = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_neighbors': [5, 7, 10, 15, 20],  # Número de vecinos\n",
        "    'weights': ['uniform', 'distance'],  # Peso de los vecinos\n",
        "    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']  # Métricas de distancia\n",
        "}\n",
        "\n",
        "grid_search_knn = GridSearchCV(\n",
        "    estimator=KNeighborsClassifier(),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # Validación cruzada\n",
        "    scoring='accuracy',  # Métrica para evaluar\n",
        "    verbose=1,\n",
        "    n_jobs=-1  # Paralelizar la búsqueda\n",
        ")\n",
        "\n",
        "# Ajustar el modelo a los datos de entrenamiento\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Mejores parámetros para KNN:\", grid_search_knn.best_params_)\n",
        "\n",
        "best_knn = grid_search_knn.best_estimator_\n",
        "y_pred_knn = best_knn.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy con el mejor KNN: {accuracy_score(y_test, y_pred_knn):.2f}\")\n",
        "print(\"Reporte de clasificación para KNN:\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "# Aplicar t-SNE solo a los datos de entrenamiento\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "indices = np.arange(len(y_test))\n",
        "correct = (y_pred_knn == y_test)\n",
        "\n",
        "all_embeddings = np.vstack((X_train, X_test))\n",
        "all_labels = np.concatenate((y_train, y_test))\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "all_tsne = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "train_tsne = all_tsne[:len(X_train)]\n",
        "test_tsne = all_tsne[len(X_train):]\n",
        "\n",
        "# Graficar en el mismo espacio\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 0), 0], train_tsne[np.where(y_train == 0), 1],\n",
        "    c='blue', marker='o', label='Labrax C (Train)', alpha=0.5\n",
        ")\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 1), 0], train_tsne[np.where(y_train == 1), 1],\n",
        "    c='green', marker='o', label='Labrax S (Train)', alpha=0.5\n",
        ")\n",
        "\n",
        "plt.scatter(\n",
        "    test_tsne[correct, 0], test_tsne[correct, 1],\n",
        "    c='black', marker='x', label='Predicción correcta (Test)', alpha=0.8\n",
        ")\n",
        "plt.scatter(\n",
        "    test_tsne[~correct, 0], test_tsne[~correct, 1],\n",
        "    c='red', marker='x', label='Predicción incorrecta (Test)', alpha=0.8\n",
        ")\n",
        "plt.title(\"t-SNE combinado: Entrenamiento y prueba\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_labels_Sclusters = text_labels_S\n",
        "text_labels_Dclusters = text_labels_D\n",
        "\n",
        "text_inputs = torch.cat([clip.tokenize(label) for label in text_labels_Sclusters]).to(device)\n",
        "text_inputs_2 = torch.cat([clip.tokenize(label) for label in text_labels_Dclusters]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_Sclusters = model.encode_text(text_inputs)\n",
        "    text_features_Dclusters = model.encode_text(text_inputs_2)\n",
        "\n",
        "text_features_Sclusters_norm = text_features_Sclusters / text_features_Sclusters.norm(dim=-1, keepdim=True)\n",
        "text_features_Dclusters_norm = text_features_Dclusters / text_features_Dclusters.norm(dim=-1, keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "color_for_user = ['yellow'] * len(XTRAIN)\n",
        "\n",
        "claves = list(dict_users_train_S.keys())\n",
        "sorted_keys = sorted(claves, key=lambda key: len(dict_users_train_S[key]), reverse=True)\n",
        "\n",
        "# Seleccionar las 5 claves más grandes\n",
        "top_5_keys = sorted_keys[:5]\n",
        "print(top_5_keys)\n",
        "\n",
        "# Obtener las posiciones de esas claves en la lista original\n",
        "top_5_positions = [claves.index(key) for key in top_5_keys]\n",
        "print(claves.index('SA193'))\n",
        "\n",
        "print(top_5_positions)\n",
        "\n",
        "top_5_lengths = [len(dict_users_train_S[claves[pos]]) for pos in top_5_positions]\n",
        "print(top_5_lengths)\n",
        "\n",
        "if claves[93] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[93]]:\n",
        "        color_for_user[indice] = 'red'\n",
        "if claves[110] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[110]]:\n",
        "        color_for_user[indice] = 'blue'\n",
        "if claves[45] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[45]]:\n",
        "        color_for_user[indice] = 'green'\n",
        "if claves[16] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[16]]:\n",
        "        color_for_user[indice] = 'cyan'\n",
        "if claves[105] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[105]]:\n",
        "        color_for_user[indice] = 'purple'\n",
        "\n",
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(train_tsne)):\n",
        "    plt.scatter(train_tsne[i, 0], train_tsne[i, 1], color=color_for_user[i] ,alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import MDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Aplicar MDS a los datos de entrenamiento\n",
        "mds = MDS(n_components=2, random_state=42)\n",
        "train_mds = mds.fit_transform(X_train)\n",
        "test_mds = mds.fit_transform(X_test)\n",
        "\n",
        "# Graficar MDS\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(train_mds[y_train == 0, 0], train_mds[y_train == 0, 1], c='blue', label='Aurata C (Train)', alpha=0.5)\n",
        "plt.scatter(train_mds[y_train == 1, 0], train_mds[y_train == 1, 1], c='green', label='Aurata S (Train)', alpha=0.5)\n",
        "plt.scatter(test_mds[correct, 0], test_mds[correct, 1], c='black', marker='x', label='Predicción correcta (Test)', alpha=0.8)\n",
        "plt.scatter(test_mds[~correct, 0], test_mds[~correct, 1], c='red', marker='x', label='Predicción incorrecta (Test)', alpha=0.8)\n",
        "plt.title(\"MDS combinado: Entrenamiento y prueba\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "n_clusters = 2\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(embeddings_S)\n",
        "\n",
        "# Graficar MDS para los clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(n_clusters):\n",
        "    plt.scatter(train_mds[(clusters == i), 0], train_mds[(clusters == i), 1],\n",
        "                label=f'Cluster {i + 1}', alpha=0.5)\n",
        "\n",
        "# Calcular los centroides en el espacio MDS\n",
        "centroids_mds = np.array([\n",
        "    train_mds[(clusters == i)].mean(axis=0) for i in range(n_clusters)\n",
        "])\n",
        "\n",
        "# Añadir los centroides\n",
        "plt.scatter(centroids_mds[:, 0], centroids_mds[:, 1], c='black', marker='x', s=100, label='Centroides')\n",
        "\n",
        "plt.title(\"Clusters k-means en el espacio MDS\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# `embedings_SText` contiene pares (embedding, text_embedding)\n",
        "# `text_labels_S` contiene los textos originales asociados a los embeddings en `text_features_S`\n",
        "\n",
        "# Convertir los datos en arrays numpy\n",
        "# Concatenar embeddings de imagen y texto antes de entrenar kmeans\n",
        "XTRAIN_array = np.array([np.concatenate((emb.numpy(), text.numpy())) for emb, text in embedings_SText])\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(XTRAIN_array)\n",
        "\n",
        "# Lista para almacenar los textos representativos\n",
        "textos_representativos = []\n",
        "\n",
        "for i in range(4):\n",
        "    # Calculamos las distancias al centroide del clúster\n",
        "    distances = pairwise_distances([kmeans.cluster_centers_[i]], XTRAIN_array)\n",
        "    closest_point_idx = np.argmin(distances)  # Índice del punto más cercano\n",
        "\n",
        "    # Recuperar el embedding de texto más cercano\n",
        "    _, closest_text_embedding = embedings_SText[closest_point_idx]\n",
        "\n",
        "    # Convertir embedding a texto original buscando el índice correspondiente en `text_features_S`\n",
        "    similarity = (closest_text_embedding @ text_norm_S.T).softmax(dim=-1)\n",
        "    text_idx = similarity.argmax().item()  # Índice del texto más similar\n",
        "    texto_representativo = text_labels_S[text_idx]  # Mapea al texto original\n",
        "\n",
        "    textos_representativos.append(texto_representativo)\n",
        "\n",
        "# Mostrar los textos representativos\n",
        "print(\"Textos representativos por clúster:\")\n",
        "for i, texto in enumerate(textos_representativos, 1):\n",
        "    print(f\"Clúster {i}: {texto}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "X_test = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "# Definir grid de hiperparámetros para Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # Validación cruzada\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1  # Paralelización\n",
        ")\n",
        "\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Mejores parámetros para Random Forest:\", grid_search_rf.best_params_)\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy con Random Forest: {accuracy_score(y_test, y_pred_rf):.2f}\")\n",
        "print(\"Reporte de clasificación para Random Forest:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "all_embeddings = np.vstack((X_train, X_test))\n",
        "all_labels = np.concatenate((y_train, y_test))\n",
        "all_tsne = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "train_tsne = all_tsne[:len(X_train)]\n",
        "test_tsne = all_tsne[len(X_train):]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 0), 0], train_tsne[np.where(y_train == 0), 1],\n",
        "    c='blue', marker='o', label='Aurata S (Train)', alpha=0.5\n",
        ")\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 1), 0], train_tsne[np.where(y_train == 1), 1],\n",
        "    c='green', marker='o', label='Aurata C (Train)', alpha=0.5\n",
        ")\n",
        "\n",
        "correct = (y_pred_rf == y_test)\n",
        "plt.scatter(\n",
        "    test_tsne[correct, 0], test_tsne[correct, 1],\n",
        "    c='black', marker='x', label='Predicción correcta (Test)', alpha=0.8\n",
        ")\n",
        "plt.scatter(\n",
        "    test_tsne[~correct, 0], test_tsne[~correct, 1],\n",
        "    c='red', marker='x', label='Predicción incorrecta (Test)', alpha=0.8\n",
        ")\n",
        "plt.title(\"t-SNE combinado: Entrenamiento y prueba\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "users_test_label_S=[]\n",
        "users_y_pred_S=[]\n",
        "\n",
        "for _, indices in dict_users_S.items():\n",
        "  users_test_label_S.append(sum([y_test[i] for i in indices])/len(indices))\n",
        "  users_y_pred_S.append(sum([y_pred_knn[i] for i in indices])/len(indices))\n",
        "\n",
        "# Discretizar los valores continuos\n",
        "users_test_label_S_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_S]\n",
        "users_y_pred_S_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_S]\n",
        "\n",
        "print(len(users_test_label_S_discrete))\n",
        "print(len(users_y_pred_S_discrete))\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "cm = confusion_matrix(users_test_label_S_discrete, users_y_pred_S_discrete)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix S_aurata')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Escalado de los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "# Configuración del GridSearchCV para SVC\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],  # Valores del parámetro de penalización\n",
        "    'gamma': [0.01, 0.1, 1, 'scale', 'auto'],  # Valores del coeficiente del kernel\n",
        "    'kernel': ['linear', 'rbf', 'poly']  # Tipos de kernel\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=SVC(),  # Modelo SVC\n",
        "    param_grid=param_grid, \n",
        "    cv=5,  # Validación cruzada con 5 particiones\n",
        "    verbose=1, \n",
        "    n_jobs=-1  # Paralelización\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "print(\"\\nMejores parámetros encontrados:\")\n",
        "print(best_params)\n",
        "\n",
        "model_svc = grid_search.best_estimator_\n",
        "y_pred = model_svc.predict(X_test_scaled)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy del mejor modelo: {accuracy:.2f}\")\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "users_test_label_S=[]\n",
        "users_y_pred_S=[]\n",
        "\n",
        "for _, indices in dict_users_S.items():\n",
        "  users_test_label_S.append(sum([y_test[i] for i in indices])/len(indices))\n",
        "  users_y_pred_S.append(sum([y_pred_knn[i] for i in indices])/len(indices))\n",
        "\n",
        "# Discretizar los valores continuos\n",
        "users_test_label_S_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_S]\n",
        "users_y_pred_S_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_S]\n",
        "\n",
        "print(len(users_test_label_S_discrete))\n",
        "print(len(users_y_pred_S_discrete))\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "cm = confusion_matrix(users_test_label_S_discrete, users_y_pred_S_discrete)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix S_aurata')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "X_test = scaler.transform(XTEST)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "print(len(XTRAIN))\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.scatter(train_tsne[(labels_S == 0), 0], train_tsne[(labels_S == 0), 1],\n",
        "            c='blue', marker='o', label='Labrax S')\n",
        "\n",
        "plt.scatter(train_tsne[(labels_S == 1), 0], train_tsne[(labels_S == 1), 1],\n",
        "            c='green', marker='o', label='Labrax C')\n",
        "\n",
        "plt.title(\"K-means Clusters in the t-SNE space\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = [\n",
        "    'red',      # Rojo\n",
        "    'blue',     # Azul\n",
        "    'green',    # Verde\n",
        "    'orange',   # Naranja\n",
        "    'purple',   # Púrpura\n",
        "    'brown',    # Marrón\n",
        "    \n",
        "    'pink',     # Rosa\n",
        "    'gray',     # Gris\n",
        "    'olive',    # Oliva\n",
        "    'cyan',     # Cian\n",
        "    'gold',     # Dorado\n",
        "    'teal'      # Verde azulado\n",
        "]\n",
        "p_colors=[]\n",
        "\n",
        "for punto in XTRAIN:\n",
        "    embIMG=torch.from_numpy(punto[:512])\n",
        "    embIMG_norm = embIMG / embIMG.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * embIMG_norm @ text_features_Sclusters_norm.T).softmax(dim=-1)\n",
        "    _ , indices = similarity.topk(6)\n",
        "    p_colors.append(colors[indices[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(train_tsne)):\n",
        "    plt.scatter(train_tsne[i, 0], train_tsne[i, 1], color=p_colors[i] ,alpha=0.5)\n",
        "\n",
        "# Calcular los centroides en el espacio t-SNE\n",
        "centroids_tsne = np.array([\n",
        "    train_tsne[(clusters == i)].mean(axis=0) for i in range(n_clusters)\n",
        "])\n",
        "\n",
        "# Añadir los centroides\n",
        "plt.scatter(centroids_tsne[:, 0], centroids_tsne[:, 1], c='black', marker='x', s=100, label='Centroids')\n",
        "\n",
        "plt.title(\"K-means Clusters in the t-SNE space\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "color_for_user = ['yellow'] * len(XTRAIN)\n",
        "\n",
        "claves = list(dict_users_train_S.keys())\n",
        "sorted_keys = sorted(claves, key=lambda key: len(dict_users_train_S[key]), reverse=True)\n",
        "\n",
        "# Seleccionar las 5 claves más grandes\n",
        "top_5_keys = sorted_keys[:5]\n",
        "print(top_5_keys)\n",
        "\n",
        "# Obtener las posiciones de esas claves en la lista original\n",
        "top_5_positions = [claves.index(key) for key in top_5_keys]\n",
        "print(claves.index('SA193'))\n",
        "\n",
        "print(top_5_positions)\n",
        "\n",
        "top_5_lengths = [len(dict_users_train_S[claves[pos]]) for pos in top_5_positions]\n",
        "print(top_5_lengths)\n",
        "\n",
        "if claves[93] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[93]]:\n",
        "        color_for_user[indice] = 'red'\n",
        "if claves[110] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[110]]:\n",
        "        color_for_user[indice] = 'blue'\n",
        "if claves[45] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[45]]:\n",
        "        color_for_user[indice] = 'green'\n",
        "if claves[16] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[16]]:\n",
        "        color_for_user[indice] = 'cyan'\n",
        "if claves[105] in dict_users_train_S:\n",
        "    for indice in dict_users_train_S[claves[105]]:\n",
        "        color_for_user[indice] = 'purple'\n",
        "\n",
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(embeddings_S)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(train_tsne)):\n",
        "    plt.scatter(train_tsne[i, 0], train_tsne[i, 1], color=color_for_user[i] ,alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(class_weights)\n",
        "\n",
        "model.add(Dense(128, input_dim=X_train.shape[1],use_bias=True))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='loss',  # Monitorea la pérdida de entrenamiento (sin validación)\n",
        "    mode='min',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='loss',\n",
        "    factor=0.1,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, callbacks=[early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Calcular pesos para clases desbalanceadas\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Crear el modelo de Linear Probe\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))  # Clasificador lineal\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks para el entrenamiento\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Entrenamiento Loss')\n",
        "#plt.plot(history.history['val_loss'], label='Validación Loss')\n",
        "plt.title('Pérdida durante el entrenamiento')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Entrenamiento Accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validación Accuracy')\n",
        "plt.title('Exactitud durante el entrenamiento')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Exactitud')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluación en el conjunto de prueba\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Predicciones en el conjunto de prueba\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# Matriz de confusión normal\n",
        "print(\"\\nMatriz de confusión (General):\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix (General)\")\n",
        "plt.show()\n",
        "\n",
        "# Reporte de clasificación\n",
        "print(\"\\nReporte de clasificación (General):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Evaluación por individuo\n",
        "users_test_label_S = []\n",
        "users_y_pred_S = []\n",
        "\n",
        "for _, indices in dict_users_S.items():\n",
        "    # Calcular promedio de etiquetas reales y predicciones por usuario\n",
        "    users_test_label_S.append(np.mean([y_test[i] for i in indices]))\n",
        "    users_y_pred_S.append(np.mean([y_pred[i] for i in indices]))\n",
        "\n",
        "# Discretizar valores continuos\n",
        "users_test_label_S_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_S]\n",
        "users_y_pred_S_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_S]\n",
        "\n",
        "# Verificar tamaños consistentes\n",
        "assert len(users_test_label_S_discrete) == len(users_y_pred_S_discrete), \"Error: Los tamaños no coinciden.\"\n",
        "\n",
        "# Matriz de confusión por individuo\n",
        "print(\"\\nMatriz de confusión (Por usuario):\")\n",
        "cm_users = confusion_matrix(users_test_label_S_discrete, users_y_pred_S_discrete)\n",
        "sns.heatmap(cm_users, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (User Level)')\n",
        "plt.show()\n",
        "\n",
        "# Métricas a nivel de usuario\n",
        "accuracy_users = accuracy_score(users_test_label_S_discrete, users_y_pred_S_discrete)\n",
        "print(f\"\\nAccuracy por usuario: {accuracy_users:.2f}\")\n",
        "print(\"\\nReporte de clasificación (Por usuario):\")\n",
        "print(classification_report(users_test_label_S_discrete, users_y_pred_S_discrete))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "XEscap_S_scaled = scaler.transform(XEscap_S)\n",
        "y_escapS = model.predict(XEscap_S_scaled)\n",
        "y_escapS = (y_escapS > 0.5).astype(int).flatten()\n",
        "count_0, count_1 = np.bincount(y_escapS)\n",
        "\n",
        "print(\"Cautivos: \"+str(count_0))\n",
        "print(\"salvajes: \"+str(count_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "users_y_pred_ES_S = []\n",
        "for _ , indices in dict_users_ES_S.items():\n",
        "    users_y_pred_ES_S.append(np.mean([y_escapS[i] for i in indices]))\n",
        "users_y_pred_ES_S_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_ES_S]\n",
        "count_0, count_1 = np.bincount(users_y_pred_ES_S_discrete)\n",
        "\n",
        "print(\"Usuarios cautivos: \"+str(count_0))\n",
        "print(\"Usuarios salvajes: \"+str(count_1))\n",
        "\n",
        "ALL_embeddings_S = np.concatenate((XTRAIN, XEscap_S), axis=0)\n",
        "\n",
        "labels_ES_Slabels_S = np.concatenate((np.zeros(len(image_features_SS)),  # Aurata S (imágenes)\n",
        "                         np.ones(len(image_features_SC)),   # Aurata C (imágenes)\n",
        "                         np.ones(len(XEscap_S))*2))  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "embeddings_S_tsne = tsne.fit_transform(ALL_embeddings_S)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_S_tsne[(labels_ES_Slabels_S == 0), 0], embeddings_S_tsne[(labels_ES_Slabels_S == 0), 1],\n",
        "            c='blue', marker='o', label='Aurata S')\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_S_tsne[(labels_ES_Slabels_S == 1), 0], embeddings_S_tsne[(labels_ES_Slabels_S == 1), 1],\n",
        "            c='green', marker='o', label='Aurata C')\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_S_tsne[(labels_ES_Slabels_S == 2), 0], embeddings_S_tsne[(labels_ES_Slabels_S == 2), 1],\n",
        "            c='red', marker='s', label='Escapados')\n",
        "\n",
        "\n",
        "plt.title(\"t-SNE de los embeddings en 2D\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# D_LABRAX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "XTRAIN=embedings_DText\n",
        "XTEST=embeddings_DText_test\n",
        "labels_D = np.concatenate((np.zeros(len(image_features_DS)),  # Aurata S (imágenes)\n",
        "                         np.ones(len(image_features_DC))), axis=0)  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "yTRAIN=labels_D\n",
        "yTEST=test_label_D\n",
        "\n",
        "XTRAIN = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in embedings_DText]\n",
        "XTEST = [np.concatenate((emb.numpy(), text.numpy())) for emb, text in embeddings_DText_test]\n",
        "\n",
        "print(np.array(XTRAIN).shape)\n",
        "\n",
        "yTRAIN = np.array(yTRAIN)\n",
        "yTEST = np.array(yTEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "X_test = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_neighbors': [5, 7, 10, 15, 20],  # Número de vecinos\n",
        "    'weights': ['uniform', 'distance'],  # Peso de los vecinos\n",
        "    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']  # Métricas de distancia\n",
        "}\n",
        "\n",
        "grid_search_knn = GridSearchCV(\n",
        "    estimator=KNeighborsClassifier(),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # Validación cruzada\n",
        "    scoring='accuracy',  # Métrica para evaluar\n",
        "    verbose=1,\n",
        "    n_jobs=-1  # Paralelizar la búsqueda\n",
        ")\n",
        "\n",
        "# Ajustar el modelo a los datos de entrenamiento\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Mejores parámetros para KNN:\", grid_search_knn.best_params_)\n",
        "\n",
        "best_knn = grid_search_knn.best_estimator_\n",
        "y_pred_knn = best_knn.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy con el mejor KNN: {accuracy_score(y_test, y_pred_knn):.2f}\")\n",
        "print(\"Reporte de clasificación para KNN:\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "# Aplicar t-SNE solo a los datos de entrenamiento\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "indices = np.arange(len(y_test))\n",
        "correct = (y_pred_knn == y_test)\n",
        "\n",
        "all_embeddings = np.vstack((X_train, X_test))\n",
        "all_labels = np.concatenate((y_train, y_test))\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "all_tsne = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "train_tsne = all_tsne[:len(X_train)]\n",
        "test_tsne = all_tsne[len(X_train):]\n",
        "\n",
        "# Graficar en el mismo espacio\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 0), 0], train_tsne[np.where(y_train == 0), 1],\n",
        "    c='blue', marker='o', label='Labrax C (Train)', alpha=0.5\n",
        ")\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 1), 0], train_tsne[np.where(y_train == 1), 1],\n",
        "    c='green', marker='o', label='Labrax S (Train)', alpha=0.5\n",
        ")\n",
        "\n",
        "plt.scatter(\n",
        "    test_tsne[correct, 0], test_tsne[correct, 1],\n",
        "    c='black', marker='x', label='Predicción correcta (Test)', alpha=0.8\n",
        ")\n",
        "plt.scatter(\n",
        "    test_tsne[~correct, 0], test_tsne[~correct, 1],\n",
        "    c='red', marker='x', label='Predicción incorrecta (Test)', alpha=0.8\n",
        ")\n",
        "plt.title(\"t-SNE combinado: Entrenamiento y prueba\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "users_test_label_D=[]\n",
        "users_y_pred_D=[]\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "  users_test_label_D.append(sum([y_test[i] for i in indices])/len(indices))\n",
        "  users_y_pred_D.append(sum([y_pred_knn[i] for i in indices])/len(indices))\n",
        "\n",
        "# Discretizar los valores continuos\n",
        "users_test_label_D_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_D]\n",
        "users_y_pred_D_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_D]\n",
        "\n",
        "print(len(users_test_label_D_discrete))\n",
        "print(len(users_y_pred_D_discrete))\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "cm = confusion_matrix(users_test_label_D_discrete, users_y_pred_D_discrete)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix D_Labrax')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Escalar los embeddings\n",
        "scaler = StandardScaler()\n",
        "embeddings_D = scaler.fit_transform(XTRAIN)\n",
        "\n",
        "# Aplicar k-means con 3 clusters\n",
        "n_clusters = 2\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(embeddings_D)\n",
        "\n",
        "# Evaluar el resultado de k-means\n",
        "ari = adjusted_rand_score(yTRAIN, clusters)\n",
        "silhouette = silhouette_score(embeddings_D, clusters)\n",
        "\n",
        "print(f\"Adjusted Rand Index (ARI): {ari:.2f}\")\n",
        "print(f\"Silhouette Score: {silhouette:.2f}\")\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(embeddings_D)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(n_clusters):\n",
        "    plt.scatter(train_tsne[(clusters == i), 0], train_tsne[(clusters == i), 1],\n",
        "                label=f'Cluster {i + 1}', alpha=0.5)\n",
        "\n",
        "# Calcular los centroides en el espacio t-SNE\n",
        "centroids_tsne = np.array([\n",
        "    train_tsne[(clusters == i)].mean(axis=0) for i in range(n_clusters)\n",
        "])\n",
        "\n",
        "# Añadir los centroides\n",
        "plt.scatter(centroids_tsne[:, 0], centroids_tsne[:, 1], c='black', marker='x', s=100, label='Centroides')\n",
        "\n",
        "plt.title(\"K-means Clusters in the t-SNE space\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Matriz de confusión entre clusters y etiquetas reales\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(yTRAIN, clusters))\n",
        "\n",
        "# Selección de múltiples textos representativos por clúster\n",
        "num_representatives = 3  # Número de textos por clúster\n",
        "textos_representativos_por_cluster = []\n",
        "XTRAIN_array = np.array([np.concatenate((emb.numpy(), text.numpy())) for emb, text in embedings_DText])\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    # Calcular las distancias al centroide del clúster\n",
        "    distances = pairwise_distances([kmeans.cluster_centers_[i]], XTRAIN_array)\n",
        "    \n",
        "    # Obtener los índices de los puntos más cercanos al centroide\n",
        "    closest_points_idx = np.argsort(distances[0])\n",
        "    selected_text_indices = set()  # Para evitar seleccionar el mismo texto más de una vez\n",
        "    \n",
        "    # Lista temporal para los textos representativos del clúster actual\n",
        "    textos_representativos = []\n",
        "    for idx in closest_points_idx:\n",
        "        if len(textos_representativos) >= num_representatives:\n",
        "            break\n",
        "        \n",
        "        # Recuperar el embedding de texto más cercano\n",
        "        _, closest_text_embedding = embedings_DText[idx]\n",
        "\n",
        "        # Convertir embedding a texto original buscando el índice correspondiente en `text_features_S`\n",
        "        similarity = (closest_text_embedding @ text_norm_D.T).softmax(dim=-1)\n",
        "        text_idx = similarity.argmax().item()  # Índice del texto más similar\n",
        "        \n",
        "        # Asegurar que el índice de texto no haya sido seleccionado antes\n",
        "        if text_idx not in selected_text_indices:\n",
        "            texto_representativo = text_labels_D[text_idx]  # Mapea al texto original\n",
        "            textos_representativos.append(texto_representativo)\n",
        "            selected_text_indices.add(text_idx)  # Marcar este índice como utilizado\n",
        "    \n",
        "    # Añadir los textos representativos del clúster a la lista principal\n",
        "    textos_representativos_por_cluster.append(textos_representativos)\n",
        "\n",
        "# Mostrar los textos representativos por clúster\n",
        "print(\"Textos representativos por clúster:\")\n",
        "for i, textos in enumerate(textos_representativos_por_cluster, 1):\n",
        "    print(f\"Clúster {i}:\")\n",
        "    for texto in textos:\n",
        "        print(f\"  - {texto}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = [\n",
        "    'red',      # Rojo\n",
        "    'blue',     # Azul\n",
        "    'green',    # Verde\n",
        "    'orange',   # Naranja\n",
        "    'purple',   # Púrpura\n",
        "    'brown',    # Marrón\n",
        "    \n",
        "    'pink',     # Rosa\n",
        "    'gray',     # Gris\n",
        "    'olive',    # Oliva\n",
        "    'cyan',     # Cian\n",
        "    'gold',     # Dorado\n",
        "    'teal'      # Verde azulado\n",
        "]\n",
        "p_colors=[]\n",
        "\n",
        "for punto in XTRAIN:\n",
        "    embIMG=torch.from_numpy(punto[:512])\n",
        "    embIMG_norm = embIMG / embIMG.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * embIMG_norm @ text_features_Dclusters_norm.T).softmax(dim=-1)\n",
        "    _ , indices = similarity.topk(6)\n",
        "    p_colors.append(colors[indices[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_train=scaler.fit_transform(XTRAIN)\n",
        "train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(train_tsne)):\n",
        "    plt.scatter(train_tsne[i, 0], train_tsne[i, 1], color=p_colors[i] ,alpha=0.5)\n",
        "\n",
        "# Calcular los centroides en el espacio t-SNE\n",
        "centroids_tsne = np.array([\n",
        "    train_tsne[(clusters == i)].mean(axis=0) for i in range(n_clusters)\n",
        "])\n",
        "\n",
        "# Añadir los centroides\n",
        "plt.scatter(centroids_tsne[:, 0], centroids_tsne[:, 1], c='black', marker='x', s=100, label='Centroids')\n",
        "\n",
        "plt.title(\"K-means Clusters in the t-SNE space\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = [\n",
        "    'blue',     # Rosa\n",
        "    'blue',     # Gris\n",
        "    'blue',    # Oliva\n",
        "    'blue',     # Cian\n",
        "    'blue',     # Dorado\n",
        "    'blue',      # Verde azulado\n",
        "    'red',      # Rojo\n",
        "    'red',     # Azul\n",
        "    'red',    # Verde\n",
        "    'red',   # Naranja\n",
        "    'red',   # Púrpura\n",
        "    'red',    # Marrón\n",
        "]\n",
        "p_colors=[]\n",
        "\n",
        "for punto in XTRAIN:\n",
        "    embIMG=torch.from_numpy(punto[:512])\n",
        "    embIMG_norm = embIMG / embIMG.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * embIMG_norm @ text_features_Dclusters_norm.T).softmax(dim=-1)\n",
        "    _ , indices = similarity.topk(12)\n",
        "    p_colors.append(colors[indices[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_S_prueba = np.concatenate((image_features_SS, image_features_SC), axis=0)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "train_tsne = tsne.fit_transform(embeddings_D)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(train_tsne)):\n",
        "    plt.scatter(train_tsne[i, 0], train_tsne[i, 1], color=p_colors[i] ,alpha=0.5)\n",
        "\n",
        "# Calcular los centroides en el espacio t-SNE\n",
        "centroids_tsne = np.array([\n",
        "    train_tsne[(clusters == i)].mean(axis=0) for i in range(n_clusters)\n",
        "])\n",
        "\n",
        "# Añadir los centroides\n",
        "plt.scatter(centroids_tsne[:, 0], centroids_tsne[:, 1], c='black', marker='x', s=100, label='Centroids')\n",
        "\n",
        "plt.title(\"K-means Clusters in the t-SNE space\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(XTRAIN)\n",
        "X_test = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "# Definir grid de hiperparámetros para Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # Validación cruzada\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1  # Paralelización\n",
        ")\n",
        "\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Mejores parámetros para Random Forest:\", grid_search_rf.best_params_)\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy con Random Forest: {accuracy_score(y_test, y_pred_rf):.2f}\")\n",
        "print(\"Reporte de clasificación para Random Forest:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "all_embeddings = np.vstack((X_train, X_test))\n",
        "all_labels = np.concatenate((y_train, y_test))\n",
        "all_tsne = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "train_tsne = all_tsne[:len(X_train)]\n",
        "test_tsne = all_tsne[len(X_train):]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 0), 0], train_tsne[np.where(y_train == 0), 1],\n",
        "    c='blue', marker='o', label='Aurata S (Train)', alpha=0.5\n",
        ")\n",
        "plt.scatter(\n",
        "    train_tsne[np.where(y_train == 1), 0], train_tsne[np.where(y_train == 1), 1],\n",
        "    c='green', marker='o', label='Aurata C (Train)', alpha=0.5\n",
        ")\n",
        "\n",
        "correct = (y_pred_rf == y_test)\n",
        "plt.scatter(\n",
        "    test_tsne[correct, 0], test_tsne[correct, 1],\n",
        "    c='black', marker='x', label='Predicción correcta (Test)', alpha=0.8\n",
        ")\n",
        "plt.scatter(\n",
        "    test_tsne[~correct, 0], test_tsne[~correct, 1],\n",
        "    c='red', marker='x', label='Predicción incorrecta (Test)', alpha=0.8\n",
        ")\n",
        "plt.title(\"t-SNE combinado: Entrenamiento y prueba\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "users_test_label_D=[]\n",
        "users_y_pred_D=[]\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "  users_test_label_D.append(sum([y_test[i] for i in indices])/len(indices))\n",
        "  users_y_pred_D.append(sum([y_pred_knn[i] for i in indices])/len(indices))\n",
        "\n",
        "# Discretizar los valores continuos\n",
        "users_test_label_D_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_D]\n",
        "users_y_pred_D_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_D]\n",
        "\n",
        "print(len(users_test_label_D_discrete))\n",
        "print(len(users_y_pred_D_discrete))\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "cm = confusion_matrix(users_test_label_D_discrete, users_y_pred_D_discrete)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix D_Labrax')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(class_weights)\n",
        "\n",
        "model.add(Dense(256, input_dim=X_train.shape[1],use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='loss',  # Monitorea la pérdida de entrenamiento (sin validación)\n",
        "    mode='min',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='loss',\n",
        "    factor=0.1,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, callbacks=[early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Calcular pesos para clases desbalanceadas\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Crear el modelo de Linear Probe\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))  # Clasificador lineal\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks para el entrenamiento\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Entrenamiento Loss')\n",
        "#plt.plot(history.history['val_loss'], label='Validación Loss')\n",
        "plt.title('Pérdida durante el entrenamiento')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Entrenamiento Accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validación Accuracy')\n",
        "plt.title('Exactitud durante el entrenamiento')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Exactitud')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluación en el conjunto de prueba\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Predicciones en el conjunto de prueba\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "# Matriz de confusión normal\n",
        "print(\"\\nMatriz de confusión (General):\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix (General)\")\n",
        "plt.show()\n",
        "\n",
        "# Reporte de clasificación\n",
        "print(\"\\nReporte de clasificación (General):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Evaluación por individuo\n",
        "users_test_label_D = []\n",
        "users_y_pred_D = []\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "    # Calcular promedio de etiquetas reales y predicciones por usuario\n",
        "    users_test_label_D.append(np.mean([y_test[i] for i in indices]))\n",
        "    users_y_pred_D.append(np.mean([y_pred[i] for i in indices]))\n",
        "\n",
        "# Discretizar valores continuos\n",
        "users_test_label_D_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_D]\n",
        "users_y_pred_D_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_D]\n",
        "\n",
        "# Verificar tamaños consistentes\n",
        "assert len(users_test_label_D_discrete) == len(users_y_pred_D_discrete), \"Error: Los tamaños no coinciden.\"\n",
        "\n",
        "# Matriz de confusión por individuo\n",
        "print(\"\\nMatriz de confusión (Por usuario):\")\n",
        "cm_users = confusion_matrix(users_test_label_D_discrete, users_y_pred_D_discrete)\n",
        "sns.heatmap(cm_users, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (User Level)')\n",
        "plt.show()\n",
        "\n",
        "# Métricas a nivel de usuario\n",
        "accuracy_users = accuracy_score(users_test_label_D_discrete, users_y_pred_D_discrete)\n",
        "print(f\"\\nAccuracy por usuario: {accuracy_users:.2f}\")\n",
        "print(\"\\nReporte de clasificación (Por usuario):\")\n",
        "print(classification_report(users_test_label_D_discrete, users_y_pred_D_discrete))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Escalado de los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(XTRAIN)\n",
        "X_test_scaled = scaler.transform(XTEST)\n",
        "y_train = np.array(yTRAIN)\n",
        "y_test = 1 - np.array(yTEST)\n",
        "\n",
        "# Configuración del GridSearchCV para SVC\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],  # Valores del parámetro de penalización\n",
        "    'gamma': [0.01, 0.1, 1, 'scale', 'auto'],  # Valores del coeficiente del kernel\n",
        "    'kernel': ['linear', 'rbf', 'poly']  # Tipos de kernel\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=SVC(),  # Modelo SVC\n",
        "    param_grid=param_grid, \n",
        "    cv=5,  # Validación cruzada con 5 particiones\n",
        "    verbose=1, \n",
        "    n_jobs=-1  # Paralelización\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "print(\"\\nMejores parámetros encontrados:\")\n",
        "print(best_params)\n",
        "\n",
        "model_svc = grid_search.best_estimator_\n",
        "y_pred = model_svc.predict(X_test_scaled)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy del mejor modelo: {accuracy:.2f}\")\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "users_test_label_D=[]\n",
        "users_y_pred_D=[]\n",
        "\n",
        "for _, indices in dict_users_D.items():\n",
        "  users_test_label_D.append(sum([y_test[i] for i in indices])/len(indices))\n",
        "  users_y_pred_D.append(sum([y_pred_knn[i] for i in indices])/len(indices))\n",
        "\n",
        "# Discretizar los valores continuos\n",
        "users_test_label_D_discrete = [1 if label >= 0.5 else 0 for label in users_test_label_D]\n",
        "users_y_pred_D_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_D]\n",
        "\n",
        "print(len(users_test_label_D_discrete))\n",
        "print(len(users_y_pred_D_discrete))\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "cm = confusion_matrix(users_test_label_D_discrete, users_y_pred_D_discrete)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix D_Labrax')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "XEscap_D_scaled = scaler.transform(XEscap_D)\n",
        "y_escapD = model.predict(XEscap_D_scaled)\n",
        "y_escapD = (y_escapD > 0.5).astype(int).flatten()\n",
        "count_0, count_1 = np.bincount(y_escapD)\n",
        "\n",
        "print(\"Cautivos: \"+str(count_0))\n",
        "print(\"salvajes: \"+str(count_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "users_y_pred_ES_D = []\n",
        "\n",
        "for _ , indices in dict_users_ES_D.items():\n",
        "    users_y_pred_ES_D.append(np.mean([y_escapD[i] for i in indices]))\n",
        "\n",
        "users_y_pred_ES_D_discrete = [1 if pred >= 0.5 else 0 for pred in users_y_pred_ES_D]\n",
        "count_0, count_1 = np.bincount(users_y_pred_ES_D_discrete)\n",
        "\n",
        "print(\"Usuarios cautivos: \"+str(count_0))\n",
        "print(\"Usuarios salvajes: \"+str(count_1))\n",
        "\n",
        "ALL_embeddings_D = np.concatenate((XTRAIN, XEscap_D), axis=0)\n",
        "\n",
        "labels_ES_Slabels_D = np.concatenate((np.zeros(len(image_features_DS)),  # Aurata S (imágenes)\n",
        "                         np.ones(len(image_features_DC)),   # Aurata C (imágenes)\n",
        "                         np.ones(len(XEscap_D))*2))  # Embeddings de texto: \"Aurata S\" y \"Aurata C\"\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "embeddings_D_tsne = tsne.fit_transform(ALL_embeddings_D)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_D_tsne[(labels_ES_Slabels_D == 0), 0], embeddings_D_tsne[(labels_ES_Slabels_D == 0), 1],\n",
        "            c='blue', marker='o', label='Labrax S')\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_D_tsne[(labels_ES_Slabels_D == 1), 0], embeddings_D_tsne[(labels_ES_Slabels_D == 1), 1],\n",
        "            c='green', marker='o', label='Labrax C')\n",
        "\n",
        "\n",
        "plt.scatter(embeddings_D_tsne[(labels_ES_Slabels_D == 2), 0], embeddings_D_tsne[(labels_ES_Slabels_D == 2), 1],\n",
        "            c='red', marker='s', label='Escapados')\n",
        "\n",
        "\n",
        "plt.title(\"t-SNE de los embeddings en 2D\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gGsURoGALRtY",
        "7e1IhLruOtDv",
        "-x-3CzxFDIvy",
        "ZFS1aw-LPK3_",
        "5pbeQZG6OJJZ",
        "gTpeGjSFCV0Z"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
